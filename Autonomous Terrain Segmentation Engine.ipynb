{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a10e14-08fb-44ac-89e2-7d3dc4f9c658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created project at: /Users/jayamdaxeshkumarshah/semnav_terrain\n",
      "Folders:\n",
      " - configs\n",
      " - data/images\n",
      " - data/labels\n",
      " - data/depth\n",
      " - data/splits\n",
      " - data/eda_samples\n",
      " - semnav\n",
      " - semnav/models\n",
      " - semnav/losses\n",
      " - semnav/metrics\n",
      " - scripts\n",
      " - runs\n",
      " - export\n",
      " - reports/figs\n",
      "\n",
      "Next: install requirements.\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Creates the folder structure exactly as in the playbook.\n",
    "# - Writes requirements.txt and minimal README.md.\n",
    "# - Creates empty/stub files for code modules we will fill later from the notebook.\n",
    "\n",
    "import os, textwrap, json\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path.cwd() / \"semnav_terrain\"\n",
    "dirs = [\n",
    "    root / \"configs\",\n",
    "    root / \"data\" / \"images\",\n",
    "    root / \"data\" / \"labels\",\n",
    "    root / \"data\" / \"depth\",\n",
    "    root / \"data\" / \"splits\",\n",
    "    root / \"data\" / \"eda_samples\",\n",
    "    root / \"semnav\",\n",
    "    root / \"semnav\" / \"models\",\n",
    "    root / \"semnav\" / \"losses\",\n",
    "    root / \"semnav\" / \"metrics\",\n",
    "    root / \"scripts\",\n",
    "    root / \"runs\",\n",
    "    root / \"export\",\n",
    "    root / \"reports\" / \"figs\",\n",
    "]\n",
    "\n",
    "for d in dirs:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# __init__ for package\n",
    "(root / \"semnav\" / \"__init__.py\").write_text(\"\")\n",
    "\n",
    "# requirements.txt (exact list from playbook)\n",
    "req = \"\"\"torch>=2.2\n",
    "torchvision>=0.17\n",
    "opencv-python\n",
    "numpy\n",
    "yaml\n",
    "rich\n",
    "tqdm\n",
    "albumentations\n",
    "matplotlib\n",
    "onnx\n",
    "onnxruntime-gpu\n",
    "\"\"\"\n",
    "(root / \"requirements.txt\").write_text(req)\n",
    "\n",
    "# Minimal configs (we will overwrite/update later)\n",
    "(root / \"configs\" / \"deeplabv3_mbv3.yaml\").write_text(textwrap.dedent(\"\"\"\\\n",
    "model: deeplabv3_mbv3\n",
    "num_classes: 7\n",
    "input_size: [512,384]\n",
    "optimizer: {name: AdamW, lr: 3e-4, weight_decay: 0.01}\n",
    "loss: {ce_weighted: true, dice: 0.3, boundary: 0.2}\n",
    "train: {epochs: 5, batch_size: 2, amp: false}   # CPU-friendly starter\n",
    "val: {tta_flip: true}\n",
    "\"\"\"))\n",
    "\n",
    "(root / \"configs\" / \"fusion_lite.yaml\").write_text(textwrap.dedent(\"\"\"\\\n",
    "model: fusion_lite\n",
    "rgb_backbone: deeplabv3_mbv3\n",
    "depth_backbone: resnet18_unetlite\n",
    "fusion_scales: [8,4]\n",
    "num_classes: 7\n",
    "\"\"\"))\n",
    "\n",
    "# Tiny README\n",
    "(root / \"README.md\").write_text(textwrap.dedent(\"\"\"\\\n",
    "# SemNav Terrain (Notebook Build)\n",
    "\n",
    "This repo is generated by a Jupyter notebook to train a small semantic segmentation model for terrain awareness.\n",
    "See the notebook for step-by-step instructions. CPU-only settings are used by default.\n",
    "\"\"\"))\n",
    "\n",
    "# Stubs for scripts; we'll fill functions in later cells or inline\n",
    "(root / \"scripts\" / \"make_splits.py\").write_text(\"# will be populated from notebook\\n\")\n",
    "(root / \"scripts\" / \"viz_samples.py\").write_text(\"# will be populated from notebook\\n\")\n",
    "\n",
    "# Stub modules; we will write actual code in later cells\n",
    "(root / \"semnav\" / \"models\" / \"deeplabv3.py\").write_text(\"# model will be written from notebook\\n\")\n",
    "(root / \"semnav\" / \"models\" / \"fusion_lite.py\").write_text(\"# model will be written from notebook\\n\")\n",
    "(root / \"semnav\" / \"losses\" / \"dice.py\").write_text(\"# dice loss from notebook\\n\")\n",
    "(root / \"semnav\" / \"losses\" / \"boundary.py\").write_text(\"# boundary loss from notebook\\n\")\n",
    "(root / \"semnav\" / \"metrics\" / \"miou.py\").write_text(\"# miou metric from notebook\\n\")\n",
    "(root / \"semnav\" / \"metrics\" / \"boundary_iou.py\").write_text(\"# boundary iou from notebook\\n\")\n",
    "(root / \"semnav\" / \"metrics\" / \"recall2m.py\").write_text(\"# recall@2m from notebook\\n\")\n",
    "\n",
    "# Train / infer stubs (we will write content later)\n",
    "(root / \"train.py\").write_text(\"# training loop will be written from notebook\\n\")\n",
    "(root / \"infer.py\").write_text(\"# inference/export code from notebook\\n\")\n",
    "(root / \"webcam_demo.py\").write_text(\"# webcam demo code from notebook\\n\")\n",
    "\n",
    "print(\"Created project at:\", root)\n",
    "print(\"Folders:\")\n",
    "for d in dirs:\n",
    "    print(\" -\", d.relative_to(root))\n",
    "print(\"\\nNext: install requirements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b44f70c-ebb6-4edd-b976-c93cdfba4ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated requirements.txt to:\n",
      "\n",
      "torch>=2.2\n",
      "torchvision>=0.17\n",
      "opencv-python\n",
      "numpy\n",
      "PyYAML\n",
      "rich\n",
      "tqdm\n",
      "albumentations\n",
      "matplotlib\n",
      "onnx\n",
      "onnxruntime\n",
      "\n",
      "\n",
      "Re-installing packages from the updated requirements...\n",
      "Requirement already satisfied: torch>=2.2 in /opt/anaconda3/lib/python3.13/site-packages (from -r semnav_terrain/requirements.txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: torchvision>=0.17 in /opt/anaconda3/lib/python3.13/site-packages (from -r semnav_terrain/requirements.txt (line 2)) (0.24.0)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.13/site-packages (from -r semnav_terrain/requirements.txt (line 3)) (4.12.0.88)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (from -r semnav_terrain/requirements.txt (line 4)) (2.1.3)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/lib/python3.13/site-packages (from -r semnav_terrain/requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.13/site-packages (from -r semnav_terrain/requirements.txt (line 6)) (13.9.4)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (from -r semnav_terrain/requirements.txt (line 7)) (4.67.1)\n",
      "Requirement already satisfied: albumentations in /opt/anaconda3/lib/python3.13/site-packages (from -r semnav_terrain/requirements.txt (line 8)) (2.0.8)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.13/site-packages (from -r semnav_terrain/requirements.txt (line 9)) (3.10.0)\n",
      "Requirement already satisfied: onnx in /opt/anaconda3/lib/python3.13/site-packages (from -r semnav_terrain/requirements.txt (line 10)) (1.19.1)\n",
      "Requirement already satisfied: onnxruntime in /opt/anaconda3/lib/python3.13/site-packages (from -r semnav_terrain/requirements.txt (line 11)) (1.23.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch>=2.2->-r semnav_terrain/requirements.txt (line 1)) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch>=2.2->-r semnav_terrain/requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch>=2.2->-r semnav_terrain/requirements.txt (line 1)) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch>=2.2->-r semnav_terrain/requirements.txt (line 1)) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch>=2.2->-r semnav_terrain/requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch>=2.2->-r semnav_terrain/requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch>=2.2->-r semnav_terrain/requirements.txt (line 1)) (2025.3.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from torchvision>=0.17->-r semnav_terrain/requirements.txt (line 2)) (11.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from rich->-r semnav_terrain/requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.13/site-packages (from rich->-r semnav_terrain/requirements.txt (line 6)) (2.19.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from albumentations->-r semnav_terrain/requirements.txt (line 8)) (1.15.3)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /opt/anaconda3/lib/python3.13/site-packages (from albumentations->-r semnav_terrain/requirements.txt (line 8)) (2.10.3)\n",
      "Requirement already satisfied: albucore==0.0.24 in /opt/anaconda3/lib/python3.13/site-packages (from albumentations->-r semnav_terrain/requirements.txt (line 8)) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /opt/anaconda3/lib/python3.13/site-packages (from albumentations->-r semnav_terrain/requirements.txt (line 8)) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /opt/anaconda3/lib/python3.13/site-packages (from albucore==0.0.24->albumentations->-r semnav_terrain/requirements.txt (line 8)) (4.2.3)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /opt/anaconda3/lib/python3.13/site-packages (from albucore==0.0.24->albumentations->-r semnav_terrain/requirements.txt (line 8)) (6.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r semnav_terrain/requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r semnav_terrain/requirements.txt (line 9)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r semnav_terrain/requirements.txt (line 9)) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r semnav_terrain/requirements.txt (line 9)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r semnav_terrain/requirements.txt (line 9)) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r semnav_terrain/requirements.txt (line 9)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r semnav_terrain/requirements.txt (line 9)) (2.9.0.post0)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /opt/anaconda3/lib/python3.13/site-packages (from onnx->-r semnav_terrain/requirements.txt (line 10)) (5.29.3)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from onnx->-r semnav_terrain/requirements.txt (line 10)) (0.5.3)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/lib/python3.13/site-packages (from onnxruntime->-r semnav_terrain/requirements.txt (line 11)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/lib/python3.13/site-packages (from onnxruntime->-r semnav_terrain/requirements.txt (line 11)) (25.9.23)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->-r semnav_terrain/requirements.txt (line 6)) (0.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic>=2.9.2->albumentations->-r semnav_terrain/requirements.txt (line 8)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic>=2.9.2->albumentations->-r semnav_terrain/requirements.txt (line 8)) (2.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib->-r semnav_terrain/requirements.txt (line 9)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.2->-r semnav_terrain/requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/lib/python3.13/site-packages (from coloredlogs->onnxruntime->-r semnav_terrain/requirements.txt (line 11)) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch>=2.2->-r semnav_terrain/requirements.txt (line 1)) (3.0.2)\n",
      "\n",
      "Done. If any package still errors, copy the error text here and we'll adjust.\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Rewrites two lines in semnav_terrain/requirements.txt:\n",
    "#     yaml            -> PyYAML\n",
    "#     onnxruntime-gpu -> onnxruntime   (CPU build for macOS/arm64)\n",
    "# - Then re-runs pip install.\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, subprocess\n",
    "\n",
    "req_path = Path(\"semnav_terrain/requirements.txt\")\n",
    "lines = [ln.strip() for ln in req_path.read_text().splitlines() if ln.strip()]\n",
    "\n",
    "fixed = []\n",
    "for ln in lines:\n",
    "    if ln.lower() == \"yaml\":\n",
    "        ln = \"PyYAML\"\n",
    "    if ln.lower() == \"onnxruntime-gpu\":\n",
    "        ln = \"onnxruntime\"\n",
    "    fixed.append(ln)\n",
    "\n",
    "req_path.write_text(\"\\n\".join(fixed) + \"\\n\")\n",
    "\n",
    "print(\"Updated requirements.txt to:\\n\")\n",
    "print(req_path.read_text())\n",
    "\n",
    "print(\"\\nRe-installing packages from the updated requirements...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_path)])\n",
    "print(\"\\nDone. If any package still errors, copy the error text here and we'll adjust.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887cbdb4-f791-48b9-af23-a85040c0ffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing webcam. Press ESC to exit.\n",
      "Closed webcam.\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Opens your default webcam.\n",
    "# - Displays a window with FPS text. Press ESC to quit.\n",
    "\n",
    "import cv2, time\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Webcam not found. Please check your camera device or permissions.\")\n",
    "\n",
    "last = time.time()\n",
    "frames = 0\n",
    "\n",
    "print(\"Showing webcam. Press ESC to exit.\")\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "    frames += 1\n",
    "    now = time.time()\n",
    "    if now - last >= 1.0:\n",
    "        fps = frames / (now - last)\n",
    "        last = now\n",
    "        frames = 0\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (12,28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
    "    cv2.imshow(\"SemNav — Webcam (Smoke Test)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Closed webcam.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455d1ffd-76bd-46ba-9c47-896751c464f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Curbs Dataset.zip -> semnav_terrain/data/raw/Curbs Dataset ...\n",
      "Done.\n",
      "Extracting Gardens Point GT.zip -> semnav_terrain/data/raw/Gardens Point GT ...\n",
      "Done.\n",
      "Extracting terrain awareness dataset.zip -> semnav_terrain/data/raw/terrain awareness dataset ...\n",
      "Done.\n",
      "Extracting Crosswalk Pixelwise Groundtruth.zip -> semnav_terrain/data/raw/Crosswalk Pixelwise Groundtruth ...\n",
      "Done.\n",
      "\n",
      "All zips extracted (if any were found). Next: standardize structure.\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Looks for dataset ZIP files in the current directory or ./zips/\n",
    "# - Extracts each to semnav_terrain/data/raw/<dataset_name>/\n",
    "\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "raw_dir = root / \"data\" / \"raw\"\n",
    "raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "search_dirs = [Path.cwd(), Path.cwd()/ \"zips\"]\n",
    "zip_files = []\n",
    "for s in search_dirs:\n",
    "    if s.exists():\n",
    "        zip_files += list(s.glob(\"*.zip\"))\n",
    "\n",
    "if not zip_files:\n",
    "    print(\"No .zip files found. Put your dataset zips in the current folder or ./zips/ and re-run.\")\n",
    "else:\n",
    "    for zf in zip_files:\n",
    "        dst = raw_dir / zf.stem\n",
    "        dst.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Extracting {zf.name} -> {dst} ...\")\n",
    "        with zipfile.ZipFile(zf, 'r') as z:\n",
    "            z.extractall(dst)\n",
    "        print(\"Done.\")\n",
    "\n",
    "print(\"\\nAll zips extracted (if any were found). Next: standardize structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283318cf-39b2-4061-a256-5c0a25ef3f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found total aligned (RGB+Label [+Depth optional]) triplets: 0\n",
      "Copied/standardized 0 samples into data/images, data/labels, data/depth (if available).\n",
      "Summary => images: 328, labels: 328, depth_png: 0, depth_npy: 0\n",
      "If counts look wrong, adjust naming patterns in the code and re-run this cell.\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Crawls data/raw/** to find RGB, Depth, Label files using simple name patterns.\n",
    "# - Builds aligned triplets by base name (without extension).\n",
    "# - Copies (or symlinks) into data/images, data/depth, data/labels with the SAME base name.\n",
    "\n",
    "import shutil, re\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "raw_dir = root / \"data\" / \"raw\"\n",
    "img_out = root / \"data\" / \"images\"\n",
    "lbl_out = root / \"data\" / \"labels\"\n",
    "dep_out = root / \"data\" / \"depth\"\n",
    "\n",
    "# helper: find files\n",
    "def list_files(p: Path):\n",
    "    return [f for f in p.rglob(\"*\") if f.is_file()]\n",
    "\n",
    "def is_image(p: Path):\n",
    "    return p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]\n",
    "\n",
    "def guess_kind(p: Path):\n",
    "    name = p.name.lower()\n",
    "    # depth: file names with 'depth' or 16-bit png (we'll just treat png with 'depth' in name)\n",
    "    if (\"depth\" in name) or (\"disp\" in name) or (\"distance\" in name):\n",
    "        return \"depth\" if p.suffix.lower() in [\".png\", \".npy\"] else \"unknown\"\n",
    "    # labels/masks\n",
    "    if any(k in name for k in [\"label\", \"mask\", \"gt\", \"annot\", \"seg\"]):\n",
    "        return \"label\" if p.suffix.lower() in [\".png\"] else \"unknown\"\n",
    "    # otherwise likely RGB image\n",
    "    if is_image(p):\n",
    "        return \"rgb\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def basekey(p: Path):\n",
    "    # remove typical suffix hints like _label, _mask\n",
    "    n = p.stem\n",
    "    n = re.sub(r\"(_label|_mask|_lbl|_seg|_gt)$\", \"\", n)\n",
    "    n = re.sub(r\"(_depth|_disp|_distance)$\", \"\", n)\n",
    "    return n\n",
    "\n",
    "files = []\n",
    "for d in raw_dir.glob(\"*\"):\n",
    "    if d.is_dir():\n",
    "        files += list_files(d)\n",
    "\n",
    "rgb_map, dep_map, lbl_map = {}, {}, {}\n",
    "for f in files:\n",
    "    kind = guess_kind(f)\n",
    "    if kind == \"unknown\":\n",
    "        continue\n",
    "    key = basekey(f)\n",
    "    if kind == \"rgb\":\n",
    "        rgb_map.setdefault(key, f)\n",
    "    elif kind == \"depth\":\n",
    "        dep_map.setdefault(key, f)\n",
    "    elif kind == \"label\":\n",
    "        lbl_map.setdefault(key, f)\n",
    "\n",
    "triplets = []\n",
    "for key in set(list(rgb_map.keys()) + list(dep_map.keys()) + list(lbl_map.keys())):\n",
    "    rgb = rgb_map.get(key, None)\n",
    "    dep = dep_map.get(key, None)\n",
    "    lbl = lbl_map.get(key, None)\n",
    "    # We require at least RGB + Label; depth optional.\n",
    "    if rgb is not None and lbl is not None:\n",
    "        triplets.append((key, rgb, dep, lbl))\n",
    "\n",
    "print(f\"Found total aligned (RGB+Label [+Depth optional]) triplets: {len(triplets)}\")\n",
    "\n",
    "# copy into unified layout\n",
    "copied = 0\n",
    "for key, rgb, dep, lbl in triplets:\n",
    "    rgb_dst = img_out / f\"{key}.png\"\n",
    "    lbl_dst = lbl_out / f\"{key}.png\"\n",
    "    shutil.copy2(rgb, rgb_dst) if rgb.suffix.lower() != \".png\" else shutil.copy2(rgb, rgb_dst)\n",
    "    shutil.copy2(lbl, lbl_dst)\n",
    "    if dep is not None:\n",
    "        dep_dst = dep_out / f\"{key}{dep.suffix.lower()}\"\n",
    "        shutil.copy2(dep, dep_dst)\n",
    "    copied += 1\n",
    "\n",
    "print(f\"Copied/standardized {copied} samples into data/images, data/labels, data/depth (if available).\")\n",
    "\n",
    "# quick summary\n",
    "num_images = len(list(img_out.glob(\"*.png\")))\n",
    "num_labels = len(list(lbl_out.glob(\"*.png\")))\n",
    "num_depth_png = len(list(dep_out.glob(\"*.png\")))\n",
    "num_depth_npy = len(list(dep_out.glob(\"*.npy\")))\n",
    "print(f\"Summary => images: {num_images}, labels: {num_labels}, depth_png: {num_depth_png}, depth_npy: {num_depth_npy}\")\n",
    "print(\"If counts look wrong, adjust naming patterns in the code and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e1fad05-a6a0-48cf-806b-bda9a4647028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATASET: Gardens Point GT ===\n",
      "RGB candidates:    0\n",
      "LABEL candidates:  1200\n",
      "DEPTH candidates:  0\n",
      "UNKNOWN image-like: 0\n",
      "\n",
      "=== DATASET: Curbs Dataset ===\n",
      "RGB candidates:    100\n",
      "LABEL candidates:  100\n",
      "DEPTH candidates:  0\n",
      "UNKNOWN image-like: 0\n",
      "\n",
      "=== DATASET: terrain awareness dataset ===\n",
      "RGB candidates:    120\n",
      "LABEL candidates:  120\n",
      "DEPTH candidates:  120\n",
      "UNKNOWN image-like: 0\n",
      "\n",
      "=== DATASET: Crosswalk Pixelwise Groundtruth ===\n",
      "RGB candidates:    191\n",
      "LABEL candidates:  191\n",
      "DEPTH candidates:  0\n",
      "UNKNOWN image-like: 0\n",
      "\n",
      "=== GRAND TOTALS ACROSS ALL RAW DATASETS ===\n",
      "RGB: 411   LABEL: 1611   DEPTH: 120   UNKNOWN(image-like): 0\n",
      "\n",
      "-- SAMPLE RGB CANDIDATES (up to 20) --\n",
      "  Curbs Dataset/Curbs Dataset/left/yuquan/13-37-52leftp/339.png\n",
      "  Curbs Dataset/Curbs Dataset/left/yuquan/13-37-52leftp/338.png\n",
      "  Curbs Dataset/Curbs Dataset/left/yuquan/13-37-52leftp/336.png\n",
      "  Curbs Dataset/Curbs Dataset/left/yuquan/13-37-52leftp/337.png\n",
      "  Curbs Dataset/Curbs Dataset/left/yuquan/13-37-52leftp/335.png\n",
      "  Curbs Dataset/Curbs Dataset/left/yuquan/13-37-52leftp/341.png\n",
      "  Curbs Dataset/Curbs Dataset/left/yuquan/13-37-52leftp/340.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/4/74.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/4/71.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/4/70.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/4/72.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/4/73.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/4/69.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/4/68.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/5/158.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/5/159.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/5/157.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/5/156.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/5/154.png\n",
      "  Curbs Dataset/Curbs Dataset/left/test2/5/155.png\n",
      "\n",
      "-- SAMPLE LABEL CANDIDATES (up to 20) --\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image108.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image120.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image134.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image097.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image083.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image068.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image054.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image040.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image041.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image055.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image069.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image082.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image096.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image135.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image121.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image109.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image137.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image123.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image080.png\n",
      "  Gardens Point GT/Gardens Point GT/leftImg8bit/dayleft/Image094.png\n",
      "\n",
      "-- SAMPLE DEPTH CANDIDATES (up to 20) --\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/69depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/281depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/429depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/213depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/56depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/6depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/325depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/383depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/423depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/100depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/37depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/345depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/122depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/492depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/493depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/400depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/155depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/333depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/449depth.png\n",
      "  terrain awareness dataset/terrain awareness dataset/depth/128depth.png\n",
      "\n",
      "-- SAMPLE UNKNOWN (image-like, not matched) (up to 20) --\n",
      "(none)\n",
      "\n",
      "Tip: If many labels are in UNKNOWN, note their extensions/keywords and we will extend the matcher.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DATASET DIAGNOSTIC — find RGB / LABEL / DEPTH candidates\n",
    "# What this cell does:\n",
    "#   1) Recursively scans semnav_terrain/data/raw/** for files.\n",
    "#   2) Classifies files as RGB / LABEL / DEPTH / UNKNOWN using broader rules.\n",
    "#   3) Prints per-dataset counts and shows sample paths it thinks match.\n",
    "#   4) Helps you see why \"0 triplets\" happened and which names we must handle.\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "raw_dir = root / \"data\" / \"raw\"\n",
    "\n",
    "if not raw_dir.exists():\n",
    "    print(\"raw dir not found:\", raw_dir.resolve())\n",
    "    print(\"Make sure you extracted your zips into semnav_terrain/data/raw/ ...\")\n",
    "else:\n",
    "    # ---- patterns we consider ----\n",
    "    IMG_EXTS   = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "    LABEL_EXTS = {\".png\", \".bmp\", \".tif\", \".tiff\"}  # (we expect masks as image files)\n",
    "    DEPTH_EXTS = {\".png\", \".npy\", \".tif\", \".tiff\", \".exr\"}  # detect, we might not read .exr\n",
    "\n",
    "    LABEL_WORDS = [\n",
    "        \"label\", \"labels\", \"mask\", \"masks\", \"gt\", \"groundtruth\",\n",
    "        \"annotation\", \"annotations\", \"seg\", \"segmentation\", \"labelids\", \"classid\", \"classids\"\n",
    "    ]\n",
    "    DEPTH_WORDS = [\n",
    "        \"depth\", \"depthmap\", \"depth_map\", \"dep\", \"disparity\", \"disp\",\n",
    "        \"distance\", \"range\", \"depthimage\", \"z\", \"zed\"\n",
    "    ]\n",
    "    RGB_HINT_WORDS = [\"image\", \"images\", \"rgb\", \"color\", \"left\", \"img\"]\n",
    "\n",
    "    # helper: return lowercase tokens from path (name + parent names)\n",
    "    def tokens_for(p: Path):\n",
    "        toks = [p.name.lower()]\n",
    "        par = p.parent\n",
    "        for _ in range(3):  # look up to 3 levels of parent names\n",
    "            toks.append(par.name.lower())\n",
    "            par = par.parent\n",
    "        return \" \".join(toks)\n",
    "\n",
    "    # classify using filename + parent folder hints\n",
    "    def classify(p: Path):\n",
    "        name = p.name.lower()\n",
    "        ex = p.suffix.lower()\n",
    "        toks = tokens_for(p)\n",
    "\n",
    "        if ex in DEPTH_EXTS and any(w in toks for w in DEPTH_WORDS):\n",
    "            return \"depth\"\n",
    "        if (ex in LABEL_EXTS) and any(w in toks for w in LABEL_WORDS):\n",
    "            return \"label\"\n",
    "        if (ex in IMG_EXTS):\n",
    "            # If filename/folders strongly say label/depth, they would've matched above.\n",
    "            # Otherwise assume it's RGB.\n",
    "            return \"rgb\"\n",
    "        return \"unknown\"\n",
    "\n",
    "    # gather per top-level dataset folder\n",
    "    dataset_dirs = [d for d in raw_dir.glob(\"*\") if d.is_dir()]\n",
    "    if not dataset_dirs:\n",
    "        print(\"No dataset subfolders under:\", raw_dir.resolve())\n",
    "    else:\n",
    "        grand_rgb, grand_lbl, grand_dep, grand_unk = 0,0,0,0\n",
    "        sample_rgb, sample_lbl, sample_dep, sample_unk = [], [], [], []\n",
    "\n",
    "        for dset in dataset_dirs:\n",
    "            rgb = []\n",
    "            lbl = []\n",
    "            dep = []\n",
    "            unk = []\n",
    "            for f in dset.rglob(\"*\"):\n",
    "                if not f.is_file():\n",
    "                    continue\n",
    "                c = classify(f)\n",
    "                if c == \"rgb\":\n",
    "                    rgb.append(f)\n",
    "                    if len(sample_rgb)<20: sample_rgb.append(f)\n",
    "                elif c == \"label\":\n",
    "                    lbl.append(f)\n",
    "                    if len(sample_lbl)<20: sample_lbl.append(f)\n",
    "                elif c == \"depth\":\n",
    "                    dep.append(f)\n",
    "                    if len(sample_dep)<20: sample_dep.append(f)\n",
    "                else:\n",
    "                    # only track \"unknown\" that look like images to help debugging\n",
    "                    if f.suffix.lower() in (IMG_EXTS | DEPTH_EXTS):\n",
    "                        unk.append(f)\n",
    "                        if len(sample_unk)<20: sample_unk.append(f)\n",
    "\n",
    "            print(\"\\n=== DATASET:\", dset.name, \"===\")\n",
    "            print(\"RGB candidates:   \", len(rgb))\n",
    "            print(\"LABEL candidates: \", len(lbl))\n",
    "            print(\"DEPTH candidates: \", len(dep))\n",
    "            print(\"UNKNOWN image-like:\", len(unk))\n",
    "\n",
    "            grand_rgb += len(rgb)\n",
    "            grand_lbl += len(lbl)\n",
    "            grand_dep += len(dep)\n",
    "            grand_unk += len(unk)\n",
    "\n",
    "        print(\"\\n=== GRAND TOTALS ACROSS ALL RAW DATASETS ===\")\n",
    "        print(\"RGB:\", grand_rgb, \"  LABEL:\", grand_lbl, \"  DEPTH:\", grand_dep, \"  UNKNOWN(image-like):\", grand_unk)\n",
    "\n",
    "        # show samples\n",
    "        def show_samples(title, arr):\n",
    "            print(f\"\\n-- {title} (up to 20) --\")\n",
    "            if not arr:\n",
    "                print(\"(none)\")\n",
    "            else:\n",
    "                for p in arr[:20]:\n",
    "                    print(\" \", p.relative_to(raw_dir))\n",
    "\n",
    "        show_samples(\"SAMPLE RGB CANDIDATES\", sample_rgb)\n",
    "        show_samples(\"SAMPLE LABEL CANDIDATES\", sample_lbl)\n",
    "        show_samples(\"SAMPLE DEPTH CANDIDATES\", sample_dep)\n",
    "        show_samples(\"SAMPLE UNKNOWN (image-like, not matched)\", sample_unk)\n",
    "\n",
    "        print(\"\\nTip: If many labels are in UNKNOWN, note their extensions/keywords and we will extend the matcher.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b3f922-b74a-430e-9b64-678f920c6b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned (RGB+Label [+Depth opt]) triplets found: 328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied/converted 328 aligned samples into:\n",
      "  images -> /Users/jayamdaxeshkumarshah/semnav_terrain/data/images\n",
      "  labels -> /Users/jayamdaxeshkumarshah/semnav_terrain/data/labels\n",
      "  depth  -> /Users/jayamdaxeshkumarshah/semnav_terrain/data/depth\n",
      "\n",
      "Examples (up to 8):\n",
      "  KEY: 0045  | RGB: 0045-color.png  | DEPTH: None  | LABEL: 0045-color.png\n",
      "  KEY: 0046  | RGB: 0046-color.png  | DEPTH: None  | LABEL: 0046-color.png\n",
      "  KEY: 0047  | RGB: 0047-color.png  | DEPTH: None  | LABEL: 0047-color.png\n",
      "  KEY: 0048  | RGB: 0048-color.png  | DEPTH: None  | LABEL: 0048-color.png\n",
      "  KEY: 0049  | RGB: 0049-color.png  | DEPTH: None  | LABEL: 0049-color.png\n",
      "  KEY: 0050  | RGB: 0050-color.png  | DEPTH: None  | LABEL: 0050-color.png\n",
      "  KEY: 0051  | RGB: 0051-color.png  | DEPTH: None  | LABEL: 0051-color.png\n",
      "  KEY: 0286  | RGB: 0286-color.png  | DEPTH: None  | LABEL: 0286-color.png\n",
      "\n",
      "SUMMARY AFTER PATCHED STANDARDIZATION\n",
      "images: 328, labels: 328, depth_png: 0, depth_npy: 0\n",
      "If counts are still low, re-run the DIAGNOSTIC cell and tell me which filenames need extra handling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PATCHED STANDARDIZER — broader matching + stem alignment\n",
    "# What this cell does:\n",
    "#   1) Uses broader rules (extensions + keywords + folder hints) to find RGB/LABEL/DEPTH.\n",
    "#   2) Normalizes base names by stripping common suffixes (e.g., _label, -mask, _left, _rgb, _depth, _disp, etc.).\n",
    "#   3) Builds aligned triplets by base name (RGB + LABEL required, DEPTH optional).\n",
    "#   4) Copies/Converts to semnav_terrain/data/{images,labels,depth}/<base>.png (depth keeps original ext if .npy).\n",
    "#   5) Prints counts and a few aligned examples.\n",
    "# Notes:\n",
    "#   - Labels that are 3-channel color masks will be copied as-is; mapping step later may need attention.\n",
    "#   - If label is 3-channel with identical channels, we auto-reduce to single channel.\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "raw_dir = root / \"data\" / \"raw\"\n",
    "img_out = root / \"data\" / \"images\"\n",
    "lbl_out = root / \"data\" / \"labels\"\n",
    "dep_out = root / \"data\" / \"depth\"\n",
    "\n",
    "img_out.mkdir(parents=True, exist_ok=True)\n",
    "lbl_out.mkdir(parents=True, exist_ok=True)\n",
    "dep_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- patterns & helpers ----\n",
    "IMG_EXTS   = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "LABEL_EXTS = {\".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "DEPTH_EXTS = {\".png\", \".npy\", \".tif\", \".tiff\", \".exr\"}\n",
    "\n",
    "LABEL_WORDS = [\n",
    "    \"label\", \"labels\", \"mask\", \"masks\", \"gt\", \"groundtruth\",\n",
    "    \"annotation\", \"annotations\", \"seg\", \"segmentation\", \"labelids\", \"classid\", \"classids\"\n",
    "]\n",
    "DEPTH_WORDS = [\n",
    "    \"depth\", \"depthmap\", \"depth_map\", \"dep\", \"disparity\", \"disp\",\n",
    "    \"distance\", \"range\", \"depthimage\", \"z\", \"zed\"\n",
    "]\n",
    "RGB_HINT_WORDS = [\"image\", \"images\", \"rgb\", \"color\", \"left\", \"img\", \"frames\"]\n",
    "\n",
    "# common suffixes to strip when forming the \"base key\"\n",
    "SUFFIXES = [\n",
    "    r\"(_|-)label(ids)?$\", r\"(_|-)labels$\", r\"(_|-)mask$\", r\"(_|-)masks$\",\n",
    "    r\"(_|-)gt$\", r\"(_|-)seg$\", r\"(_|-)segmentation$\", r\"(_|-)annotation(s)?$\",\n",
    "    r\"(_|-)class(id|ids)$\",\n",
    "    r\"(_|-)rgb$\", r\"(_|-)left$\", r\"(_|-)image$\", r\"(_|-)img$\", r\"(_|-)color$\",\n",
    "    r\"(_|-)depth(map)?$\", r\"(_|-)disp(arit(y)?)?$\", r\"(_|-)distance$\", r\"(_|-)range$\", r\"(_|-)z$\"\n",
    "]\n",
    "SUFFIX_REGEX = re.compile(\"(\" + \"|\".join(SUFFIXES) + \")\", re.IGNORECASE)\n",
    "\n",
    "def tokens_for(p: Path):\n",
    "    toks = [p.name.lower()]\n",
    "    par = p.parent\n",
    "    for _ in range(3):\n",
    "        toks.append(par.name.lower())\n",
    "        par = par.parent\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def classify(p: Path):\n",
    "    ex = p.suffix.lower()\n",
    "    toks = tokens_for(p)\n",
    "    # depth first\n",
    "    if (ex in DEPTH_EXTS) and any(w in toks for w in DEPTH_WORDS):\n",
    "        return \"depth\"\n",
    "    # labels next\n",
    "    if (ex in LABEL_EXTS) and any(w in toks for w in LABEL_WORDS):\n",
    "        return \"label\"\n",
    "    # otherwise if looks like an image, call it rgb\n",
    "    if ex in IMG_EXTS:\n",
    "        return \"rgb\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def make_basekey(p: Path):\n",
    "    stem = p.stem\n",
    "    stem = SUFFIX_REGEX.sub(\"\", stem)  # strip defined suffixes\n",
    "    stem = re.sub(r\"[\\s]+\", \"_\", stem) # collapse spaces to underscore\n",
    "    return stem\n",
    "\n",
    "# scan all files\n",
    "files = [f for f in raw_dir.rglob(\"*\") if f.is_file()]\n",
    "rgb_map, lbl_map, dep_map = {}, {}, {}\n",
    "\n",
    "for f in files:\n",
    "    c = classify(f)\n",
    "    if c == \"unknown\":\n",
    "        continue\n",
    "    key = make_basekey(f)\n",
    "    # prefer files located in conventional folders by overriding prior picks\n",
    "    score = 0\n",
    "    toks = tokens_for(f)\n",
    "    if c == \"rgb\":\n",
    "        if any(w in toks for w in [\"image\", \"images\", \"rgb\", \"color\", \"left\", \"img\", \"frames\"]):\n",
    "            score += 1\n",
    "        prev = rgb_map.get(key)\n",
    "        if (prev is None) or (\"score\" in prev and score > prev[\"score\"]):\n",
    "            rgb_map[key] = {\"path\": f, \"score\": score}\n",
    "    elif c == \"label\":\n",
    "        if any(w in toks for w in [\"label\", \"labels\", \"gt\", \"groundtruth\", \"seg\", \"segmentation\", \"annotation\"]):\n",
    "            score += 1\n",
    "        prev = lbl_map.get(key)\n",
    "        if (prev is None) or (\"score\" in prev and score > prev[\"score\"]):\n",
    "            lbl_map[key] = {\"path\": f, \"score\": score}\n",
    "    elif c == \"depth\":\n",
    "        if any(w in toks for w in [\"depth\", \"disparity\", \"range\", \"distance\"]):\n",
    "            score += 1\n",
    "        prev = dep_map.get(key)\n",
    "        if (prev is None) or (\"score\" in prev and score > prev[\"score\"]):\n",
    "            dep_map[key] = {\"path\": f, \"score\": score}\n",
    "\n",
    "# build triplets\n",
    "keys_all = sorted(set(rgb_map.keys()) | set(lbl_map.keys()) | set(dep_map.keys()))\n",
    "triplets = []\n",
    "for k in keys_all:\n",
    "    rgb = rgb_map.get(k, {}).get(\"path\")\n",
    "    lbl = lbl_map.get(k, {}).get(\"path\")\n",
    "    dep = dep_map.get(k, {}).get(\"path\")\n",
    "    if (rgb is not None) and (lbl is not None):\n",
    "        triplets.append((k, rgb, dep, lbl))\n",
    "\n",
    "print(f\"Aligned (RGB+Label [+Depth opt]) triplets found: {len(triplets)}\")\n",
    "\n",
    "# copy/convert helpers\n",
    "def save_rgb_to_png(src: Path, dst: Path):\n",
    "    img = cv2.imread(str(src), cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        return False\n",
    "    cv2.imwrite(str(dst), img)\n",
    "    return True\n",
    "\n",
    "def save_label_to_png(src: Path, dst: Path):\n",
    "    m = cv2.imread(str(src), cv2.IMREAD_UNCHANGED)\n",
    "    if m is None:\n",
    "        return False\n",
    "    # if 3-channel and all channels equal, reduce to single-channel\n",
    "    if m.ndim == 3 and m.shape[2] == 3:\n",
    "        if np.allclose(m[:,:,0], m[:,:,1]) and np.allclose(m[:,:,1], m[:,:,2]):\n",
    "            m = m[:,:,0]\n",
    "        # else: keep 3ch; mapping step may need manual handling\n",
    "    cv2.imwrite(str(dst), m)\n",
    "    return True\n",
    "\n",
    "def copy_depth(src: Path, dst_png: Path, dst_npy: Path):\n",
    "    # keep .npy as .npy; others to .png\n",
    "    if src.suffix.lower() == \".npy\":\n",
    "        shutil.copy2(src, dst_npy)\n",
    "        return True\n",
    "    else:\n",
    "        d = cv2.imread(str(src), cv2.IMREAD_UNCHANGED)\n",
    "        if d is None:\n",
    "            # if OpenCV can't read (e.g., EXR not supported), just copy the raw file with its ext\n",
    "            shutil.copy2(src, dst_png.with_suffix(src.suffix.lower()))\n",
    "            return True\n",
    "        cv2.imwrite(str(dst_png), d)\n",
    "        return True\n",
    "\n",
    "# perform copy/convert\n",
    "copied = 0\n",
    "examples = []\n",
    "for key, rgb, dep, lbl in triplets:\n",
    "    rgb_dst = img_out / f\"{key}.png\"\n",
    "    lbl_dst = lbl_out / f\"{key}.png\"\n",
    "    ok1 = save_rgb_to_png(rgb, rgb_dst)\n",
    "    ok2 = save_label_to_png(lbl, lbl_dst)\n",
    "    ok3 = True\n",
    "    if dep is not None:\n",
    "        dep_png = dep_out / f\"{key}.png\"\n",
    "        dep_npy = dep_out / f\"{key}.npy\"\n",
    "        ok3 = copy_depth(dep, dep_png, dep_npy)\n",
    "    if ok1 and ok2 and ok3:\n",
    "        copied += 1\n",
    "        if len(examples) < 8:\n",
    "            examples.append((key, rgb.name, (dep.name if dep is not None else \"None\"), lbl.name))\n",
    "\n",
    "print(f\"Copied/converted {copied} aligned samples into:\")\n",
    "print(\"  images ->\", img_out.resolve())\n",
    "print(\"  labels ->\", lbl_out.resolve())\n",
    "print(\"  depth  ->\", dep_out.resolve())\n",
    "\n",
    "print(\"\\nExamples (up to 8):\")\n",
    "for e in examples:\n",
    "    print(\"  KEY:\", e[0], \" | RGB:\", e[1], \" | DEPTH:\", e[2], \" | LABEL:\", e[3])\n",
    "\n",
    "# final summary\n",
    "num_images = len(list(img_out.glob(\"*.png\")))\n",
    "num_labels = len(list(lbl_out.glob(\"*.png\")))\n",
    "num_depth_png = len(list(dep_out.glob(\"*.png\")))\n",
    "num_depth_npy = len(list(dep_out.glob(\"*.npy\")))\n",
    "print(\"\\nSUMMARY AFTER PATCHED STANDARDIZATION\")\n",
    "print(f\"images: {num_images}, labels: {num_labels}, depth_png: {num_depth_png}, depth_npy: {num_depth_npy}\")\n",
    "print(\"If counts are still low, re-run the DIAGNOSTIC cell and tell me which filenames need extra handling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74502f4b-1860-4ee7-83ef-627f36911aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique label IDs found across datasets: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]\n",
      "Template saved to: semnav_terrain/data/label_mapping.json\n",
      ">> IMPORTANT: Open data/label_mapping.json and edit 'source_to_target' so IDs map correctly.\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Scans all label masks to collect unique pixel values.\n",
    "# - Writes a mapping template JSON: data/label_mapping.json\n",
    "# - You can edit that JSON to map source IDs to our target {0..6}, others to 255 (ignore).\n",
    "\n",
    "import numpy as np\n",
    "import cv2, json\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "lbl_dir = root / \"data\" / \"labels\"\n",
    "mapping_path = root / \"data\" / \"label_mapping.json\"\n",
    "\n",
    "unique_ids = set()\n",
    "lbl_files = list(lbl_dir.glob(\"*.png\"))\n",
    "for i, f in enumerate(lbl_files):\n",
    "    m = cv2.imread(str(f), cv2.IMREAD_UNCHANGED)\n",
    "    if m is None:\n",
    "        continue\n",
    "    unique_ids.update(np.unique(m).tolist())\n",
    "\n",
    "unique_ids = sorted(list(unique_ids))\n",
    "print(\"Unique label IDs found across datasets:\", unique_ids)\n",
    "\n",
    "# Build default mapping: unknown -> 255, and try a simple guess (often 0=background/ground)\n",
    "default_map = {str(int(k)): 255 for k in unique_ids}\n",
    "# Give a naive guess if some standard IDs appear; you will likely need to edit\n",
    "# target IDs: ground=0, sidewalk=1, stairs=2, water=3, person=4, car=5, sky=6\n",
    "# We'll leave all as 255 except try to map '0' to ground if exists:\n",
    "if \"0\" in default_map:\n",
    "    default_map[\"0\"] = 0\n",
    "\n",
    "mapping = {\n",
    "    \"target_names\": [\"ground\",\"sidewalk\",\"stairs\",\"water\",\"person\",\"car\",\"sky\"],\n",
    "    \"ignore_index\": 255,\n",
    "    \"source_to_target\": default_map\n",
    "}\n",
    "with open(mapping_path, \"w\") as f:\n",
    "    json.dump(mapping, f, indent=2)\n",
    "\n",
    "print(f\"Template saved to: {mapping_path}\")\n",
    "print(\">> IMPORTANT: Open data/label_mapping.json and edit 'source_to_target' so IDs map correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be538082-5c9e-43f8-8e2f-c84236356d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repaired images: 0\n",
      "Repaired labels: 0\n",
      "Removed unreadable samples: 0\n",
      "Remaining counts: images 328 labels 328 depth_png 0 depth_npy 0\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Scans semnav_terrain/data/images and semnav_terrain/data/labels by key.\n",
    "# - Tries to read each PNG with cv2.imread; if it fails, tries cv2.imdecode() from raw bytes and re-saves.\n",
    "# - If still unreadable, removes that key from images/labels/depth so later steps won't crash.\n",
    "# - Prints how many files were repaired and how many samples were removed.\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "img_dir = root / \"data\" / \"images\"\n",
    "lbl_dir = root / \"data\" / \"labels\"\n",
    "dep_dir = root / \"data\" / \"depth\"\n",
    "\n",
    "def try_read(path):\n",
    "    arr = cv2.imread(str(path), cv2.IMREAD_UNCHANGED)\n",
    "    if arr is not None:\n",
    "        return arr, \"imread\"\n",
    "    # try imdecode from bytes (sometimes helps)\n",
    "    try:\n",
    "        raw = np.fromfile(str(path), dtype=np.uint8)\n",
    "        if raw.size > 0:\n",
    "            arr2 = cv2.imdecode(raw, cv2.IMREAD_UNCHANGED)\n",
    "            if arr2 is not None:\n",
    "                return arr2, \"imdecode\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "def reencode_png(path, arr):\n",
    "    try:\n",
    "        ok = cv2.imwrite(str(path), arr)\n",
    "        return bool(ok)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def remove_key(key):\n",
    "    removed = []\n",
    "    for folder, ext in [(img_dir,\".png\"), (lbl_dir,\".png\"),\n",
    "                        (dep_dir,\".png\"), (dep_dir,\".npy\")]:\n",
    "        p = folder / f\"{key}{ext}\"\n",
    "        if p.exists():\n",
    "            try:\n",
    "                p.unlink()\n",
    "                removed.append(p)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return removed\n",
    "\n",
    "# collect keys by intersection of filenames we expect\n",
    "img_keys = {p.stem for p in img_dir.glob(\"*.png\")}\n",
    "lbl_keys = {p.stem for p in lbl_dir.glob(\"*.png\")}\n",
    "all_keys = sorted(img_keys | lbl_keys)\n",
    "\n",
    "repaired_imgs = 0\n",
    "repaired_lbls = 0\n",
    "removed_samples = 0\n",
    "bad_keys = []\n",
    "\n",
    "for key in all_keys:\n",
    "    ip = img_dir / f\"{key}.png\"\n",
    "    lp = lbl_dir / f\"{key}.png\"\n",
    "\n",
    "    # check image\n",
    "    if ip.exists():\n",
    "        img, how = try_read(ip)\n",
    "        if img is None:\n",
    "            # cannot read, try to drop this key\n",
    "            bad_keys.append(key)\n",
    "        elif how == \"imdecode\":\n",
    "            if reencode_png(ip, img):\n",
    "                repaired_imgs += 1\n",
    "\n",
    "    # check label\n",
    "    if lp.exists():\n",
    "        lbl, how = try_read(lp)\n",
    "        if lbl is None:\n",
    "            bad_keys.append(key)\n",
    "        elif how == \"imdecode\":\n",
    "            if reencode_png(lp, lbl):\n",
    "                repaired_lbls += 1\n",
    "\n",
    "# finalize: remove keys that are still unreadable\n",
    "bad_keys = sorted(set(bad_keys))\n",
    "for key in bad_keys:\n",
    "    removed = remove_key(key)\n",
    "    if removed:\n",
    "        removed_samples += 1\n",
    "\n",
    "print(f\"Repaired images: {repaired_imgs}\")\n",
    "print(f\"Repaired labels: {repaired_lbls}\")\n",
    "print(f\"Removed unreadable samples: {removed_samples}\")\n",
    "print(\"Remaining counts:\",\n",
    "      \"images\", len(list(img_dir.glob('*.png'))),\n",
    "      \"labels\", len(list(lbl_dir.glob('*.png'))),\n",
    "      \"depth_png\", len(list(dep_dir.glob('*.png'))),\n",
    "      \"depth_npy\", len(list(dep_dir.glob('*.npy'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "279445c5-a937-46fa-944e-2da0b471ad6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied mapping to 328 labels. Skipped unreadables: 0\n",
      "Total valid aligned samples (RGB+Label) used for splits: 328  (skipped invalid: 0)\n",
      "Fold 0: train=67 val=14 test=15\n",
      "Fold 1: train=40 val=8 test=10\n",
      "Fold 2: train=40 val=8 test=10\n",
      "Fold 3: train=40 val=8 test=10\n",
      "Fold 4: train=40 val=8 test=10\n",
      "5-fold splits saved in data/splits/.\n"
     ]
    }
   ],
   "source": [
    "# SAFE replacement for your original \"Cell 1D — Apply mapping + 5-fold splits\"\n",
    "# - Skips any unreadable labels (None) so we never call .any() on a bool.\n",
    "# - If a label is 3-channel with identical channels, squeezes to 1 channel for presence computation.\n",
    "# - Writes folds only for valid samples.\n",
    "\n",
    "import json, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "img_dir = root / \"data\" / \"images\"\n",
    "lbl_dir = root / \"data\" / \"labels\"\n",
    "dep_dir = root / \"data\" / \"depth\"\n",
    "splits_dir = root / \"data\" / \"splits\"\n",
    "splits_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mapping_path = root / \"data\" / \"label_mapping.json\"\n",
    "if not mapping_path.exists():\n",
    "    raise RuntimeError(\"label_mapping.json not found. Run the label mapping cell first (or the fix cell that creates it).\")\n",
    "\n",
    "with open(mapping_path, \"r\") as f:\n",
    "    mapping = json.load(f)\n",
    "ignore_index = int(mapping.get(\"ignore_index\",255))\n",
    "source_to_target = {int(k): int(v) for k,v in mapping[\"source_to_target\"].items()}\n",
    "num_classes = 7\n",
    "\n",
    "# 1) Apply mapping to all labels (in place), skipping unreadables\n",
    "lbl_files = sorted(lbl_dir.glob(\"*.png\"))\n",
    "mapped_ok = 0\n",
    "skipped = 0\n",
    "for f in lbl_files:\n",
    "    m = cv2.imread(str(f), cv2.IMREAD_UNCHANGED)\n",
    "    if m is None:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    if m.ndim == 3:\n",
    "        # if 3-channel but identical channels, squeeze to 1\n",
    "        if np.allclose(m[:,:,0], m[:,:,1]) and np.allclose(m[:,:,1], m[:,:,2]):\n",
    "            m = m[:,:,0]\n",
    "        else:\n",
    "            # leave as is; mapping below will operate on the first channel to stay safe here\n",
    "            m = m[:,:,0]\n",
    "    flat = m.reshape(-1)\n",
    "    out = np.full(flat.shape, ignore_index, dtype=np.uint8)\n",
    "    for s_id, t_id in source_to_target.items():\n",
    "        out[flat == s_id] = t_id\n",
    "    out = out.reshape(m.shape)\n",
    "    cv2.imwrite(str(f), out)\n",
    "    mapped_ok += 1\n",
    "\n",
    "print(f\"Applied mapping to {mapped_ok} labels. Skipped unreadables: {skipped}\")\n",
    "\n",
    "# 2) Build sample list & class presence (skip unreadables)\n",
    "img_files = sorted(img_dir.glob(\"*.png\"))\n",
    "samples = []\n",
    "bad = 0\n",
    "for f in img_files:\n",
    "    key = f.stem\n",
    "    lbl = lbl_dir / f\"{key}.png\"\n",
    "    if not lbl.exists():\n",
    "        continue\n",
    "    m = cv2.imread(str(lbl), cv2.IMREAD_UNCHANGED)\n",
    "    if m is None:\n",
    "        bad += 1\n",
    "        continue\n",
    "    if m.ndim == 3:\n",
    "        # safety squeeze first channel\n",
    "        m = m[:,:,0]\n",
    "    present = np.zeros(num_classes, dtype=np.int32)\n",
    "    for c in range(num_classes):\n",
    "        present[c] = int(np.any(m == c))\n",
    "    has_depth = (dep_dir / f\"{key}.png\").exists() or (dep_dir / f\"{key}.npy\").exists()\n",
    "    samples.append({\"key\": key, \"present\": present, \"has_depth\": has_depth})\n",
    "\n",
    "print(f\"Total valid aligned samples (RGB+Label) used for splits: {len(samples)}  (skipped invalid: {bad})\")\n",
    "\n",
    "# 3) 5-fold approximate stratification\n",
    "K = 5\n",
    "random.seed(42)\n",
    "random.shuffle(samples)\n",
    "fold_bins = [[] for _ in range(K)]\n",
    "fold_counts = np.zeros((K, num_classes), dtype=np.int32)\n",
    "\n",
    "for s in samples:\n",
    "    pres = s[\"present\"]\n",
    "    scores = []\n",
    "    for k in range(K):\n",
    "        scores.append(int((fold_counts[k] * pres).sum()))\n",
    "    k_best = int(np.argmin(scores))\n",
    "    fold_bins[k_best].append(s)\n",
    "    fold_counts[k_best] += pres\n",
    "\n",
    "def split_fold(bin_list, seed=42):\n",
    "    random.Random(seed).shuffle(bin_list)\n",
    "    n = len(bin_list)\n",
    "    n_train = int(0.7*n)\n",
    "    n_val = int(0.15*n)\n",
    "    train = bin_list[:n_train]\n",
    "    val = bin_list[n_train:n_train+n_val]\n",
    "    test = bin_list[n_train+n_val:]\n",
    "    return train, val, test\n",
    "\n",
    "for k in range(K):\n",
    "    train, val, test = split_fold(fold_bins[k], seed=100+k)\n",
    "    for name, lst in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
    "        outp = splits_dir / f\"fold{k}_{name}.txt\"\n",
    "        with open(outp, \"w\") as f:\n",
    "            for s in lst:\n",
    "                f.write(s[\"key\"] + \"\\n\")\n",
    "    print(f\"Fold {k}: train={len(train)} val={len(val)} test={len(test)}\")\n",
    "\n",
    "print(\"5-fold splits saved in data/splits/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4684e6e8-6f4c-4d0a-a55f-8318a5c12051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No depth files found to compute stats.\n",
      "Saved 12 triplet previews to: semnav_terrain/data/eda_samples\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Computes rough depth stats on a subset (clip [0.5m,10m], simple median blur).\n",
    "# - Saves 12 random RGB/Depth/Mask triplets for sanity check in data/eda_samples/.\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2, matplotlib.pyplot as plt\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "img_dir = root / \"data\" / \"images\"\n",
    "lbl_dir = root / \"data\" / \"labels\"\n",
    "dep_dir = root / \"data\" / \"depth\"\n",
    "eda_dir = root / \"data\" / \"eda_samples\"\n",
    "eda_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_depth_any(png_or_npy: Path):\n",
    "    if png_or_npy.suffix.lower() == \".npy\":\n",
    "        d = np.load(str(png_or_npy))\n",
    "    else:\n",
    "        # assume 16-bit depth if PNG; some datasets store mm\n",
    "        d = cv2.imread(str(png_or_npy), cv2.IMREAD_UNCHANGED)\n",
    "    return d\n",
    "\n",
    "# collect keys with depth\n",
    "keys = []\n",
    "for img in img_dir.glob(\"*.png\"):\n",
    "    key = img.stem\n",
    "    lbl = lbl_dir / f\"{key}.png\"\n",
    "    if not lbl.exists():\n",
    "        continue\n",
    "    d1 = dep_dir / f\"{key}.png\"\n",
    "    d2 = dep_dir / f\"{key}.npy\"\n",
    "    if d1.exists() or d2.exists():\n",
    "        keys.append(key)\n",
    "\n",
    "depth_stats = []\n",
    "for key in keys[:min(40, len(keys))]:\n",
    "    dpath = dep_dir / f\"{key}.png\"\n",
    "    if not dpath.exists():\n",
    "        dpath = dep_dir / f\"{key}.npy\"\n",
    "    d = load_depth_any(dpath)\n",
    "    if d is None:\n",
    "        continue\n",
    "    d = d.astype(np.float32)\n",
    "    # heuristic: if values look large (e.g., mm), convert to meters\n",
    "    if d.max() > 50.0:\n",
    "        d_m = d / 1000.0\n",
    "    else:\n",
    "        d_m = d\n",
    "    # clip and median blur (convert to 16-bit for medianBlur if needed)\n",
    "    d_m = np.clip(d_m, 0.5, 10.0)\n",
    "    d_show = (d_m - 0.5) / (10.0 - 0.5 + 1e-6)\n",
    "    d_u16 = (d_show * 65535.0).astype(np.uint16)\n",
    "    d_u16 = cv2.medianBlur(d_u16, 5)\n",
    "    d_m2 = d_u16.astype(np.float32) / 65535.0 * (10.0 - 0.5) + 0.5\n",
    "    depth_stats.append([float(d_m2.min()), float(np.median(d_m2)), float(d_m2.max())])\n",
    "\n",
    "if depth_stats:\n",
    "    mins, meds, maxs = zip(*depth_stats)\n",
    "    print(f\"Depth stats (sampled): min={np.mean(mins):.2f}m, median={np.mean(meds):.2f}m, max={np.mean(maxs):.2f}m\")\n",
    "else:\n",
    "    print(\"No depth files found to compute stats.\")\n",
    "\n",
    "# Save 12 triplets\n",
    "all_keys = [p.stem for p in img_dir.glob(\"*.png\")]\n",
    "random.seed(77)\n",
    "sel = random.sample(all_keys, min(12, len(all_keys)))\n",
    "\n",
    "for key in sel:\n",
    "    rgb = cv2.cvtColor(cv2.imread(str(img_dir / f\"{key}.png\")), cv2.COLOR_BGR2RGB)\n",
    "    lbl = cv2.imread(str(lbl_dir / f\"{key}.png\"), cv2.IMREAD_UNCHANGED)\n",
    "    dpath = dep_dir / f\"{key}.png\"\n",
    "    if not dpath.exists(): dpath = dep_dir / f\"{key}.npy\"\n",
    "    if dpath.exists():\n",
    "        d = load_depth_any(dpath)\n",
    "        if d is not None:\n",
    "            d = d.astype(np.float32)\n",
    "            if d.max() > 50.0:\n",
    "                d = d / 1000.0\n",
    "            d = np.clip(d, 0.5, 10.0)\n",
    "            d_show = (d - 0.5) / (10.0 - 0.5 + 1e-6)\n",
    "        else:\n",
    "            d_show = None\n",
    "    else:\n",
    "        d_show = None\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12,4))\n",
    "    ax[0].imshow(rgb); ax[0].set_title(\"RGB\"); ax[0].axis('off')\n",
    "    ax[1].imshow(lbl, vmin=0, vmax=6); ax[1].set_title(\"Label\"); ax[1].axis('off')\n",
    "    if d_show is not None:\n",
    "        ax[2].imshow(d_show, vmin=0, vmax=1); ax[2].set_title(\"Depth (~norm)\"); ax[2].axis('off')\n",
    "    else:\n",
    "        ax[2].imshow(np.zeros_like(lbl)); ax[2].set_title(\"Depth: N/A\"); ax[2].axis('off')\n",
    "    outp = eda_dir / f\"{key}_triplet.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outp)\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"Saved {len(sel)} triplet previews to: {eda_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79a86b17-f5da-4628-a17a-b67bb6fde9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforms ready: train (geom+rgb color), val/test (geom only).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/7h574p2n2pg8r4mrdg5lhzxc0000gn/T/ipykernel_26620/1769518667.py:13: UserWarning: Argument(s) 'value, mask_value' are not valid for transform PadIfNeeded\n",
      "  A.PadIfNeeded(min_height=INPUT_H, min_width=INPUT_W, border_mode=0, value=0, mask_value=255),\n",
      "/opt/anaconda3/lib/python3.13/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/var/folders/xn/7h574p2n2pg8r4mrdg5lhzxc0000gn/T/ipykernel_26620/1769518667.py:15: UserWarning: Argument(s) 'value, mask_value' are not valid for transform ShiftScaleRotate\n",
      "  A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.5, rotate_limit=5, border_mode=0, value=0, mask_value=255, p=0.7),\n",
      "/var/folders/xn/7h574p2n2pg8r4mrdg5lhzxc0000gn/T/ipykernel_26620/1769518667.py:20: UserWarning: Argument(s) 'value, mask_value' are not valid for transform PadIfNeeded\n",
      "  A.PadIfNeeded(min_height=INPUT_H, min_width=INPUT_W, border_mode=0, value=0, mask_value=255),\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Sets up Albumentations transforms.\n",
    "# - Train: resize to 512x384 (keep aspect with pad), flip, color jitter (RGB only), small rotation, scale.\n",
    "# - Val/Test: resize only.\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "INPUT_W, INPUT_H = 512, 384\n",
    "\n",
    "# Geometric transforms shared by RGB/Depth/Label\n",
    "geom_train = A.Compose([\n",
    "    A.LongestMaxSize(max_size=max(INPUT_W, INPUT_H)),\n",
    "    A.PadIfNeeded(min_height=INPUT_H, min_width=INPUT_W, border_mode=0, value=0, mask_value=255),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.5, rotate_limit=5, border_mode=0, value=0, mask_value=255, p=0.7),\n",
    "], additional_targets={'depth': 'image', 'mask': 'mask'})\n",
    "\n",
    "geom_val = A.Compose([\n",
    "    A.LongestMaxSize(max_size=max(INPUT_W, INPUT_H)),\n",
    "    A.PadIfNeeded(min_height=INPUT_H, min_width=INPUT_W, border_mode=0, value=0, mask_value=255),\n",
    "], additional_targets={'depth': 'image', 'mask': 'mask'})\n",
    "\n",
    "# Color transforms for RGB only\n",
    "color_train_rgb = A.Compose([\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=0.8)\n",
    "])\n",
    "\n",
    "print(\"Transforms ready: train (geom+rgb color), val/test (geom only).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a338ca2-5ffa-47ff-b133-4a804c7b57e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitizer summary:\n",
      " - labels squeezed to 1ch: 0\n",
      " - labels resized to match RGB: 2\n",
      " - labels clamped to {0..6,255}: 0\n",
      " - depth resized: 0\n",
      " - samples dropped: 0\n",
      "Counts now -> images: 328 labels: 328 depth_png: 0 depth_npy: 0\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Ensures every label matches its image HxW (resizes label with nearest).\n",
    "# - Converts 3-channel masks to single-channel.\n",
    "# - Clamps label values to {0..6, 255} (others -> 255 ignore).\n",
    "# - (Optional) If you later add depth, it will resize depth to match RGB too.\n",
    "# - Prints a summary of fixes and drops any sample that remains unreadable.\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "img_dir = root / \"data\" / \"images\"\n",
    "lbl_dir = root / \"data\" / \"labels\"\n",
    "dep_dir = root / \"data\" / \"depth\"\n",
    "\n",
    "valid_ids = set([0,1,2,3,4,5,6,255])\n",
    "\n",
    "def read_any(path):\n",
    "    arr = cv2.imread(str(path), cv2.IMREAD_UNCHANGED)\n",
    "    if arr is not None:\n",
    "        return arr\n",
    "    # try raw decode fallback\n",
    "    try:\n",
    "        raw = np.fromfile(str(path), dtype=np.uint8)\n",
    "        if raw.size > 0:\n",
    "            arr2 = cv2.imdecode(raw, cv2.IMREAD_UNCHANGED)\n",
    "            return arr2\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def drop_key(key):\n",
    "    removed = []\n",
    "    for folder, exts in [(img_dir, [\".png\"]),\n",
    "                         (lbl_dir, [\".png\"]),\n",
    "                         (dep_dir, [\".png\",\".npy\",\".tif\",\".tiff\",\".exr\"])]:\n",
    "        for ext in exts:\n",
    "            p = folder / f\"{key}{ext}\"\n",
    "            if p.exists():\n",
    "                try:\n",
    "                    p.unlink()\n",
    "                    removed.append(p)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return removed\n",
    "\n",
    "fixed_size = 0\n",
    "squeezed = 0\n",
    "clamped = 0\n",
    "fixed_depth = 0\n",
    "dropped = 0\n",
    "\n",
    "# iterate over keys present in images or labels\n",
    "keys = sorted({p.stem for p in img_dir.glob(\"*.png\")} | {p.stem for p in lbl_dir.glob(\"*.png\")})\n",
    "\n",
    "for key in keys:\n",
    "    ip = img_dir / f\"{key}.png\"\n",
    "    lp = lbl_dir / f\"{key}.png\"\n",
    "    if not (ip.exists() and lp.exists()):\n",
    "        # must have both rgb and label\n",
    "        _ = drop_key(key)\n",
    "        dropped += 1\n",
    "        continue\n",
    "\n",
    "    img = read_any(ip)\n",
    "    lbl = read_any(lp)\n",
    "    if img is None or lbl is None:\n",
    "        _ = drop_key(key)\n",
    "        dropped += 1\n",
    "        continue\n",
    "\n",
    "    # Ensure label single-channel\n",
    "    if lbl.ndim == 3:\n",
    "        # if gray replicated into 3 channels, squeeze; else take first channel\n",
    "        if np.allclose(lbl[:,:,0], lbl[:,:,1]) and np.allclose(lbl[:,:,1], lbl[:,:,2]):\n",
    "            lbl = lbl[:,:,0]\n",
    "        else:\n",
    "            lbl = lbl[:,:,0]\n",
    "        squeezed += 1\n",
    "\n",
    "    # Resize label to image HxW if mismatch\n",
    "    H, W = img.shape[:2]\n",
    "    if lbl.shape[:2] != (H, W):\n",
    "        lbl = cv2.resize(lbl, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "        fixed_size += 1\n",
    "\n",
    "    # Ensure uint8\n",
    "    if lbl.dtype != np.uint8:\n",
    "        lbl = lbl.astype(np.uint8)\n",
    "\n",
    "    # Clamp label IDs to {0..6,255}\n",
    "    uniq = np.unique(lbl)\n",
    "    if any(int(v) not in valid_ids for v in uniq):\n",
    "        bad_mask = ~np.isin(lbl, np.array(list(valid_ids), dtype=np.uint8))\n",
    "        lbl[bad_mask] = 255\n",
    "        clamped += 1\n",
    "\n",
    "    # write back\n",
    "    cv2.imwrite(str(lp), lbl)\n",
    "\n",
    "    # (Optional) if depth exists, resize to image HxW too\n",
    "    dp = None\n",
    "    for ext in [\".png\", \".tif\", \".tiff\", \".exr\"]:\n",
    "        if (dep_dir / f\"{key}{ext}\").exists():\n",
    "            dp = dep_dir / f\"{key}{ext}\"\n",
    "            break\n",
    "    if dp is None and (dep_dir / f\"{key}.npy\").exists():\n",
    "        dp = dep_dir / f\"{key}.npy\"\n",
    "\n",
    "    if dp is not None:\n",
    "        if dp.suffix.lower() == \".npy\":\n",
    "            d = np.load(str(dp))\n",
    "            if d.shape[:2] != (H, W):\n",
    "                d = cv2.resize(d.astype(np.float32), (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "                np.save(str(dp), d)\n",
    "                fixed_depth += 1\n",
    "        else:\n",
    "            d = read_any(dp)\n",
    "            if d is not None and d.shape[:2] != (H, W):\n",
    "                d = cv2.resize(d, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "                cv2.imwrite(str(dp), d)\n",
    "                fixed_depth += 1\n",
    "\n",
    "print(\"Sanitizer summary:\")\n",
    "print(\" - labels squeezed to 1ch:\", squeezed)\n",
    "print(\" - labels resized to match RGB:\", fixed_size)\n",
    "print(\" - labels clamped to {0..6,255}:\", clamped)\n",
    "print(\" - depth resized:\", fixed_depth)\n",
    "print(\" - samples dropped:\", dropped)\n",
    "\n",
    "# Show current counts\n",
    "print(\"Counts now ->\",\n",
    "      \"images:\", len(list(img_dir.glob('*.png'))),\n",
    "      \"labels:\", len(list(lbl_dir.glob('*.png'))),\n",
    "      \"depth_png:\", len(list(dep_dir.glob('*.png'))),\n",
    "      \"depth_npy:\", len(list(dep_dir.glob('*.npy'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73a68c2c-d8a0-4242-bfd9-13d3b745b430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes => x_rgb: (2, 3, 384, 512) x_depth: (2, 1, 384, 512) y: (2, 384, 512)\n",
      "First keys: ['300', '157']\n",
      "Train batch size (drop_last=True): 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 2B — Dataloaders (with drop_last=True to avoid 1-sample BN issues)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, cv2, os, yaml\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "img_dir = root / \"data\" / \"images\"\n",
    "lbl_dir = root / \"data\" / \"labels\"\n",
    "dep_dir = root / \"data\" / \"depth\"\n",
    "splits_dir = root / \"data\" / \"splits\"\n",
    "\n",
    "# Load config (for num_classes, batch_size, etc.)\n",
    "cfg_path = root / \"configs\" / \"deeplabv3_mbv3.yaml\"\n",
    "with open(cfg_path, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "num_classes = int(cfg.get(\"num_classes\", 7))\n",
    "bs_cfg = int(cfg.get(\"train\", {}).get(\"batch_size\", 2))\n",
    "# Ensure train batch size >= 2 to avoid BatchNorm error\n",
    "BATCH_SIZE = max(2, bs_cfg)\n",
    "\n",
    "# Albumentations transforms must be defined in Cell 2A:\n",
    "# geom_train, geom_val, color_train_rgb\n",
    "\n",
    "# Palette (BGR in OpenCV order for overlay later)\n",
    "PALETTE = np.array([\n",
    "  [128,64,128],  # ground\n",
    "  [244,35,232],  # sidewalk\n",
    "  [70,70,70],    # stairs\n",
    "  [0,0,142],     # water\n",
    "  [220,20,60],   # person\n",
    "  [0,0,230],     # car\n",
    "  [70,130,180],  # sky\n",
    "], dtype=np.uint8)\n",
    "\n",
    "def to_tensor_img(img):  # HWC BGR -> CHW RGB norm\n",
    "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)/255.0\n",
    "    mean = np.array([0.485,0.456,0.406], dtype=np.float32)\n",
    "    std  = np.array([0.229,0.224,0.225], dtype=np.float32)\n",
    "    rgb = (rgb - mean)/std\n",
    "    chw = np.transpose(rgb, (2,0,1))\n",
    "    return torch.from_numpy(chw)\n",
    "\n",
    "def to_tensor_depth(d):\n",
    "    d = d.astype(np.float32)\n",
    "    if d.max() > 50.0:  # likely mm\n",
    "        d = d / 1000.0\n",
    "    d = np.clip(d, 0.5, 10.0)\n",
    "    d = (d - 0.5) / (10.0 - 0.5 + 1e-6)  # 0..1\n",
    "    if d.ndim == 2:\n",
    "        d = d[None, ...]\n",
    "    elif d.ndim == 3:\n",
    "        d = d.transpose(2,0,1)[0:1]\n",
    "    return torch.from_numpy(d)\n",
    "\n",
    "class SemNavDataset(Dataset):\n",
    "    def __init__(self, keys, split=\"train\"):\n",
    "        self.keys = keys\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        img = cv2.imread(str(img_dir / f\"{key}.png\"))\n",
    "        mask = cv2.imread(str(lbl_dir / f\"{key}.png\"), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        # Ensure label single-channel (safety)\n",
    "        if mask is None:\n",
    "            raise RuntimeError(f\"Label missing or unreadable for key: {key}\")\n",
    "        if mask.ndim == 3:\n",
    "            if np.allclose(mask[:,:,0], mask[:,:,1]) and np.allclose(mask[:,:,1], mask[:,:,2]):\n",
    "                mask = mask[:,:,0]\n",
    "            else:\n",
    "                mask = mask[:,:,0]\n",
    "        mask = mask.astype(np.int64)\n",
    "\n",
    "        # Read depth if present\n",
    "        if (dep_dir / f\"{key}.png\").exists():\n",
    "            depth_path = dep_dir / f\"{key}.png\"\n",
    "            depth = cv2.imread(str(depth_path), cv2.IMREAD_UNCHANGED)\n",
    "        elif (dep_dir / f\"{key}.npy\").exists():\n",
    "            depth_path = dep_dir / f\"{key}.npy\"\n",
    "            depth = np.load(str(depth_path))\n",
    "        else:\n",
    "            depth = None\n",
    "\n",
    "        # Albumentations pipelines defined in Cell 2A\n",
    "        if self.split == \"train\":\n",
    "            if depth is None:\n",
    "                out = geom_train(image=img, mask=mask)\n",
    "                img_aug, mask_aug = out['image'], out['mask']\n",
    "                img_aug = color_train_rgb(image=img_aug)['image']\n",
    "            else:\n",
    "                out = geom_train(image=img, mask=mask, depth=depth)\n",
    "                img_aug, mask_aug, depth = out['image'], out['mask'], out['depth']\n",
    "                img_aug = color_train_rgb(image=img_aug)['image']\n",
    "        else:\n",
    "            if depth is None:\n",
    "                out = geom_val(image=img, mask=mask)\n",
    "                img_aug, mask_aug = out['image'], out['mask']\n",
    "            else:\n",
    "                out = geom_val(image=img, mask=mask, depth=depth)\n",
    "                img_aug, mask_aug, depth = out['image'], out['mask'], out['depth']\n",
    "\n",
    "        x_rgb = to_tensor_img(img_aug)\n",
    "        y = torch.from_numpy(mask_aug.astype(np.int64))\n",
    "        if depth is None:\n",
    "            x_depth = torch.zeros((1, y.shape[0], y.shape[1]), dtype=torch.float32)\n",
    "        else:\n",
    "            x_depth = to_tensor_depth(depth)\n",
    "\n",
    "        return x_rgb, x_depth, y, key\n",
    "\n",
    "# Load fold 0 splits (adjust k if needed)\n",
    "k = 0\n",
    "with open(splits_dir / f\"fold{k}_train.txt\") as f:\n",
    "    train_keys = [l.strip() for l in f if l.strip()]\n",
    "with open(splits_dir / f\"fold{k}_val.txt\") as f:\n",
    "    val_keys = [l.strip() for l in f if l.strip()]\n",
    "with open(splits_dir / f\"fold{k}_test.txt\") as f:\n",
    "    test_keys = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "train_ds = SemNavDataset(train_keys, split=\"train\")\n",
    "val_ds   = SemNavDataset(val_keys,   split=\"val\")\n",
    "test_ds  = SemNavDataset(test_keys,  split=\"test\")\n",
    "\n",
    "# Build DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True,     # <<< important: prevents 1-sample batch (BN error in ASPP)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Quick batch check\n",
    "xb, db, yb, kb = next(iter(train_loader))\n",
    "print(\"Batch shapes => x_rgb:\", tuple(xb.shape), \"x_depth:\", tuple(db.shape), \"y:\", tuple(yb.shape))\n",
    "print(\"First keys:\", list(kb)[:min(2, len(kb))])\n",
    "print(\"Train batch size (drop_last=True):\", xb.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12c5b073-f19b-443c-8b96-a34d76da9832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved config to: /Users/jayamdaxeshkumarshah/semnav_terrain/configs/deeplabv3_mbv3.yaml\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Rewrites configs/deeplabv3_mbv3.yaml with the playbook values (CPU-friendly epochs/batch).\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "cfg_text = textwrap.dedent(\"\"\"\\\n",
    "model: deeplabv3_mbv3\n",
    "num_classes: 7\n",
    "input_size: [512,384]\n",
    "optimizer: {name: AdamW, lr: 3e-4, weight_decay: 0.01}\n",
    "loss: {ce_weighted: true, dice: 0.3, boundary: 0.2}\n",
    "train: {epochs: 5, batch_size: 2, amp: false}\n",
    "val: {tta_flip: true}\n",
    "\"\"\")\n",
    "p = Path(\"semnav_terrain/configs/deeplabv3_mbv3.yaml\")\n",
    "p.write_text(cfg_text)\n",
    "print(\"Saved config to:\", p.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7da3df3-9189-4b2a-a0ad-19a287f4c24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote model + losses + metrics code into semnav/ package.\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Implements a DeepLabV3(+ approx) with MobileNetV3-Large backbone.\n",
    "# - Implements Dice loss and a simple Boundary-aware CE loss.\n",
    "# - Saves code into semnav/ files so train.py can import them.\n",
    "\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "model_code = textwrap.dedent(r\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
    "\n",
    "class DeeplabV3_MBV3(nn.Module):\n",
    "    def __init__(self, num_classes=7, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.net = deeplabv3_mobilenet_v3_large(weights='DEFAULT' if pretrained else None)\n",
    "        # replace classifier to our num_classes\n",
    "        in_ch = self.net.classifier[-1].in_channels\n",
    "        self.net.classifier[-1] = nn.Conv2d(in_ch, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)  # dict with 'out'\n",
    "        return out['out']\n",
    "\"\"\")\n",
    "\n",
    "dice_code = textwrap.dedent(r\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0, ignore_index=255):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, target, num_classes=7):\n",
    "        # logits: [B,C,H,W], target: [B,H,W]\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        masks = []\n",
    "        valid = (target != self.ignore_index)\n",
    "        for c in range(num_classes):\n",
    "            gt_c = (target == c) & valid\n",
    "            pred_c = probs[:, c, :, :]\n",
    "            gt_c = gt_c.float()\n",
    "            inter = (pred_c * gt_c).sum(dim=(1,2))\n",
    "            union = pred_c.sum(dim=(1,2)) + gt_c.sum(dim=(1,2))\n",
    "            dice = (2*inter + self.smooth) / (union + self.smooth)\n",
    "            masks.append(1 - dice)\n",
    "        loss = torch.stack(masks, dim=1).mean()\n",
    "        return loss\n",
    "\"\"\")\n",
    "\n",
    "boundary_code = textwrap.dedent(r\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simple boundary-aware weighting: upweight pixels whose 3x3 neighborhood contains label changes\n",
    "class BoundaryLoss(nn.Module):\n",
    "    def __init__(self, ignore_index=255, weight=1.0):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.base_ce = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='none')\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        # logits [B,C,H,W], target [B,H,W]\n",
    "        ce = self.base_ce(logits, target)  # [B,H,W]\n",
    "        with torch.no_grad():\n",
    "            # boundary mask: true if any neighbor differs\n",
    "            pad = F.pad(target.unsqueeze(1).float(), (1,1,1,1), mode='replicate')\n",
    "            patches = F.unfold(pad, kernel_size=3, stride=1)  # [B, 9, H*W]\n",
    "            center = patches[:, 4:5, :]  # center pixel\n",
    "            diff = (patches != center).float().sum(dim=1)  # [B, H*W]\n",
    "            boundary = (diff > 0).float().view_as(ce)\n",
    "            # ignore invalid areas\n",
    "            boundary[target==self.ignore_index] = 0.0\n",
    "            weights = 1.0 + 2.0*boundary  # 3x weight at boundary pixels\n",
    "        loss = (ce * weights).mean()\n",
    "        return self.weight * loss\n",
    "\"\"\")\n",
    "\n",
    "miou_code = textwrap.dedent(r\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_confusion_matrix(pred, target, num_classes=7, ignore_index=255):\n",
    "    # pred/target [B,H,W]\n",
    "    mask = (target != ignore_index)\n",
    "    pred = pred[mask]\n",
    "    target = target[mask]\n",
    "    cm = torch.zeros((num_classes, num_classes), dtype=torch.int64, device=pred.device)\n",
    "    k = target * num_classes + pred\n",
    "    binc = torch.bincount(k, minlength=num_classes**2)\n",
    "    cm += binc.reshape((num_classes, num_classes))\n",
    "    return cm\n",
    "\n",
    "def compute_miou_from_cm(cm):\n",
    "    # cm: [C,C]\n",
    "    cm = cm.float()\n",
    "    tp = torch.diag(cm)\n",
    "    denom = cm.sum(1) + cm.sum(0) - tp + 1e-6\n",
    "    iou = tp / denom\n",
    "    miou = iou.mean()\n",
    "    return miou.item(), iou.cpu().numpy()\n",
    "\"\"\")\n",
    "\n",
    "boundary_iou_code = textwrap.dedent(r\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# proxy boundary IoU using morphological edges on class=free-space (ground+sidewalk) vs prediction\n",
    "def boundary_iou_free(pred, target, free_ids=(0,1), ignore_index=255):\n",
    "    # Make binary free-space masks\n",
    "    p_free = torch.zeros_like(pred, dtype=torch.uint8)\n",
    "    t_free = torch.zeros_like(target, dtype=torch.uint8)\n",
    "    for fid in free_ids:\n",
    "        p_free |= (pred == fid)\n",
    "        t_free |= (target == fid)\n",
    "    # edges: XOR of dilated and eroded => approximate boundary\n",
    "    # Using 3x3 dilation/erosion via maxpool/minpool style operations\n",
    "    def edge_mask(x):\n",
    "        x = x.float().unsqueeze(1)\n",
    "        dil = F.max_pool2d(x, 3, stride=1, padding=1)\n",
    "        ero = -F.max_pool2d(-x, 3, stride=1, padding=1)\n",
    "        e = (dil - ero).clamp(0,1).squeeze(1).bool()\n",
    "        return e\n",
    "    pe = edge_mask(p_free)\n",
    "    te = edge_mask(t_free)\n",
    "    inter = (pe & te).sum().item()\n",
    "    union = (pe | te).sum().item() + 1e-6\n",
    "    return inter / union\n",
    "\"\"\")\n",
    "\n",
    "recall2m_code = textwrap.dedent(r\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Approximate Recall@2m for obstacle classes {person(4), car(5), stairs(2), water(3)}\n",
    "def recall_at_2m(pred, depth_m, classes=(4,5,2,3), thresh=2.0, ignore_index=255):\n",
    "    # pred: [H,W], depth_m: [H,W] in meters (float)\n",
    "    if depth_m is None:\n",
    "        return None\n",
    "    mask_close = (depth_m <= thresh)\n",
    "    if mask_close.sum() == 0:\n",
    "        return 1.0\n",
    "    obs_pred = np.zeros_like(pred, dtype=bool)\n",
    "    for c in classes:\n",
    "        obs_pred |= (pred == c)\n",
    "    tp = (obs_pred & mask_close)\n",
    "    recall = tp.sum() / (mask_close.sum() + 1e-6)\n",
    "    return float(recall)\n",
    "\"\"\")\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "(root / \"semnav\" / \"models\" / \"deeplabv3.py\").write_text(model_code)\n",
    "(root / \"semnav\" / \"losses\" / \"dice.py\").write_text(dice_code)\n",
    "(root / \"semnav\" / \"losses\" / \"boundary.py\").write_text(boundary_code)\n",
    "(root / \"semnav\" / \"metrics\" / \"miou.py\").write_text(miou_code)\n",
    "(root / \"semnav\" / \"metrics\" / \"boundary_iou.py\").write_text(boundary_iou_code)\n",
    "(root / \"semnav\" / \"metrics\" / \"recall2m.py\").write_text(recall2m_code)\n",
    "\n",
    "print(\"Wrote model + losses + metrics code into semnav/ package.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68bcc44c-58bc-4f23-a474-752601d5fd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 0 color masks to single-channel IDs.\n",
      "Applied source→target mapping to 328 labels.\n",
      "Unique target IDs now present (sampled):\n",
      "[0, 255]\n",
      "180.png: shape=(720, 1280), dtype=uint8, uniques=[np.uint8(0), np.uint8(255)]\n",
      "472color.png: shape=(480, 640), dtype=uint8, uniques=[np.uint8(0), np.uint8(255)]\n",
      "3200.png: shape=(720, 1280), dtype=uint8, uniques=[np.uint8(0), np.uint8(255)]\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# 1) Converts any 3-channel color masks in semnav_terrain/data/labels to single-channel integer ID maps.\n",
    "#    - Builds/uses a persistent color→index mapping so the same color gets the same ID across files.\n",
    "# 2) Ensures data/label_mapping.json exists; if missing, creates a template that maps found IDs to 255 except maybe 0→0.\n",
    "# 3) Applies the source→target mapping so labels end up in {0..6} with 255 as ignore.\n",
    "# 4) Prints a quick summary and sample checks.\n",
    "\n",
    "import json, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "lbl_dir = root / \"data\" / \"labels\"\n",
    "mapping_path = root / \"data\" / \"label_mapping.json\"\n",
    "color_index_map_path = root / \"data\" / \"auto_color_to_index.json\"\n",
    "\n",
    "lbl_files = sorted(lbl_dir.glob(\"*.png\"))\n",
    "if not lbl_files:\n",
    "    raise RuntimeError(\"No label PNGs found in semnav_terrain/data/labels. Did standardization run?\")\n",
    "\n",
    "# --- Step 1: Convert 3-channel color masks to single-channel index masks ---\n",
    "# persistent { \"b,g,r\": idx }\n",
    "if color_index_map_path.exists():\n",
    "    color_map = json.loads(color_index_map_path.read_text())\n",
    "else:\n",
    "    color_map = {}\n",
    "\n",
    "next_idx = 0\n",
    "if color_map:\n",
    "    next_idx = 1 + max(int(v) for v in color_map.values())\n",
    "\n",
    "converted = 0\n",
    "suspicious = 0\n",
    "\n",
    "for f in lbl_files:\n",
    "    m = cv2.imread(str(f), cv2.IMREAD_UNCHANGED)\n",
    "    if m is None:\n",
    "        continue\n",
    "\n",
    "    # If already single-channel, skip\n",
    "    if m.ndim == 2:\n",
    "        continue\n",
    "\n",
    "    # If 3-channel color mask: map each color to an integer index\n",
    "    if m.ndim == 3 and m.shape[2] == 3:\n",
    "        # If channels are identical (gray stored as 3ch), squeeze\n",
    "        if np.allclose(m[:,:,0], m[:,:,1]) and np.allclose(m[:,:,1], m[:,:,2]):\n",
    "            m_id = m[:,:,0].copy()\n",
    "        else:\n",
    "            # Build color → id using persistent global mapping\n",
    "            H, W, _ = m.shape\n",
    "            b = m[:,:,0].astype(np.uint32)\n",
    "            g = m[:,:,1].astype(np.uint32)\n",
    "            r = m[:,:,2].astype(np.uint32)\n",
    "            code = (b << 16) | (g << 8) | r  # unique code per BGR color\n",
    "\n",
    "            # Collect unique colors in this image\n",
    "            uniq = np.unique(code)\n",
    "            # Heuristic warning: too many unique colors likely means this wasn't a discrete mask\n",
    "            if uniq.size > 64:\n",
    "                suspicious += 1\n",
    "\n",
    "            # Assign IDs for any new colors\n",
    "            for c in uniq.tolist():\n",
    "                key = f\"{(c>>16)&255},{(c>>8)&255},{c&255}\"  # \"b,g,r\"\n",
    "                if key not in color_map:\n",
    "                    color_map[key] = int(next_idx)\n",
    "                    next_idx += 1\n",
    "\n",
    "            # Vectorized map from code → id\n",
    "            # Build lookup table (dict) for this file's colors\n",
    "            lut = {}\n",
    "            for c in uniq.tolist():\n",
    "                key = f\"{(c>>16)&255},{(c>>8)&255},{c&255}\"\n",
    "                lut[c] = color_map[key]\n",
    "\n",
    "            # Apply lut\n",
    "            flat = code.reshape(-1)\n",
    "            out = np.empty_like(flat, dtype=np.uint16)\n",
    "            for c in uniq.tolist():\n",
    "                out[flat == c] = lut[c]\n",
    "            m_id = out.reshape(H, W).astype(np.uint8)\n",
    "\n",
    "        # Overwrite label with single-channel ID image\n",
    "        cv2.imwrite(str(f), m_id)\n",
    "        converted += 1\n",
    "\n",
    "# Save/refresh persistent color map\n",
    "with open(color_index_map_path, \"w\") as fp:\n",
    "    json.dump(color_map, fp, indent=2)\n",
    "\n",
    "print(f\"Converted {converted} color masks to single-channel IDs.\")\n",
    "if suspicious > 0:\n",
    "    print(f\"WARNING: {suspicious} label(s) had >64 unique colors; they might not be true masks. Verify those datasets.\")\n",
    "\n",
    "# --- Step 2: Ensure label_mapping.json exists (source→target) ---\n",
    "# After conversion, current labels contain compact integer source IDs (from color map or original).\n",
    "# We need to map those source IDs to our target {0..6} with 255 ignore.\n",
    "# If mapping file doesn't exist, create a template with found IDs → 255 except maybe 0→0.\n",
    "\n",
    "# Collect unique source IDs now present\n",
    "unique_ids = set()\n",
    "for f in lbl_files:\n",
    "    m = cv2.imread(str(f), cv2.IMREAD_UNCHANGED)\n",
    "    if m is None:\n",
    "        continue\n",
    "    if m.ndim != 2:\n",
    "        # if anything still multi-channel, squeeze first channel as a last-resort\n",
    "        m = m[:,:,0]\n",
    "        cv2.imwrite(str(f), m)\n",
    "    unique_ids.update(np.unique(m).tolist())\n",
    "unique_ids = sorted(int(x) for x in unique_ids)\n",
    "\n",
    "if mapping_path.exists():\n",
    "    mapping = json.loads(mapping_path.read_text())\n",
    "else:\n",
    "    # Create a fresh template\n",
    "    default_map = {str(int(k)): 255 for k in unique_ids}\n",
    "    if \"0\" in default_map:\n",
    "        default_map[\"0\"] = 0  # common case: 0 is background/ground\n",
    "    mapping = {\n",
    "        \"target_names\": [\"ground\",\"sidewalk\",\"stairs\",\"water\",\"person\",\"car\",\"sky\"],\n",
    "        \"ignore_index\": 255,\n",
    "        \"source_to_target\": default_map\n",
    "    }\n",
    "    with open(mapping_path, \"w\") as f:\n",
    "        json.dump(mapping, f, indent=2)\n",
    "    print(\"Created template mapping at data/label_mapping.json. Edit it if needed.\")\n",
    "\n",
    "# --- Step 3: Apply source→target mapping to all labels (in place) ---\n",
    "ignore_index = int(mapping.get(\"ignore_index\", 255))\n",
    "s2t = {int(k): int(v) for k, v in mapping[\"source_to_target\"].items()}\n",
    "\n",
    "remapped = 0\n",
    "for f in lbl_files:\n",
    "    m = cv2.imread(str(f), cv2.IMREAD_UNCHANGED)\n",
    "    if m is None: \n",
    "        continue\n",
    "    if m.ndim != 2:\n",
    "        m = m[:,:,0]\n",
    "    flat = m.reshape(-1)\n",
    "    out = np.full_like(flat, ignore_index, dtype=np.uint8)\n",
    "    for s_id, t_id in s2t.items():\n",
    "        out[flat == s_id] = t_id\n",
    "    out = out.reshape(m.shape)\n",
    "    cv2.imwrite(str(f), out)\n",
    "    remapped += 1\n",
    "\n",
    "print(f\"Applied source→target mapping to {remapped} labels.\")\n",
    "print(\"Unique target IDs now present (sampled):\")\n",
    "check_ids = set()\n",
    "for f in random.sample(lbl_files, min(8, len(lbl_files))):\n",
    "    m = cv2.imread(str(f), cv2.IMREAD_UNCHANGED)\n",
    "    check_ids.update(np.unique(m).tolist())\n",
    "print(sorted(int(x) for x in check_ids)[:30])\n",
    "\n",
    "# --- Step 4: Spot-check a couple of masks to ensure 2D integer format ---\n",
    "samples = random.sample(lbl_files, min(3, len(lbl_files)))\n",
    "for s in samples:\n",
    "    m = cv2.imread(str(s), cv2.IMREAD_UNCHANGED)\n",
    "    print(f\"{s.name}: shape={m.shape}, dtype={m.dtype}, uniques={sorted(list(np.unique(m)))[:12]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b27f00c-99cb-448d-8819-e716ad97ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: /Users/jayamdaxeshkumarshah/semnav_terrain\n",
      "Imported 'semnav' from: /Users/jayamdaxeshkumarshah/semnav_terrain/semnav\n",
      "Imported DeeplabV3_MBV3 OK.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3B-fix — make 'semnav' importable for this session\n",
    "\n",
    "import sys, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "# <-- If your project lives elsewhere, change this path:\n",
    "project_root = Path(\"/Users/jayamdaxeshkumarshah/semnav_terrain\")\n",
    "\n",
    "# sanity check\n",
    "pkg_init = project_root / \"semnav\" / \"__init__.py\"\n",
    "assert pkg_init.exists(), f\"Didn't find semnav at {pkg_init} — adjust project_root above.\"\n",
    "\n",
    "# put project on Python path (front)\n",
    "pr = str(project_root.resolve())\n",
    "if pr not in sys.path:\n",
    "    sys.path.insert(0, pr)\n",
    "print(\"Added to sys.path:\", sys.path[0])\n",
    "\n",
    "# reload import caches and smoke test\n",
    "importlib.invalidate_caches()\n",
    "import semnav\n",
    "print(\"Imported 'semnav' from:\", Path(semnav.__file__).parent)\n",
    "\n",
    "# optional: test model import\n",
    "from semnav.models.deeplabv3 import DeeplabV3_MBV3\n",
    "print(\"Imported DeeplabV3_MBV3 OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a7a61df-af90-4bbd-bb58-f98737a47a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [train]: 100%|████████████| 33/33 [00:37<00:00,  1.15s/it, loss=0.766]\n",
      "Epoch 1/5 [val]: 100%|████████████████████████████| 7/7 [00:02<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val mIoU = 0.1186\n",
      ">> Saved new best to semnav_terrain/runs/fold0/best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [train]: 100%|████████████| 33/33 [00:38<00:00,  1.16s/it, loss=0.393]\n",
      "Epoch 2/5 [val]: 100%|████████████████████████████| 7/7 [00:02<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: val mIoU = 0.1265\n",
      ">> Saved new best to semnav_terrain/runs/fold0/best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [train]: 100%|██████████████| 33/33 [00:37<00:00,  1.15s/it, loss=nan]\n",
      "Epoch 3/5 [val]: 100%|████████████████████████████| 7/7 [00:02<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: val mIoU = 0.1361\n",
      ">> Saved new best to semnav_terrain/runs/fold0/best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [train]: 100%|██████████████| 33/33 [00:37<00:00,  1.15s/it, loss=nan]\n",
      "Epoch 4/5 [val]: 100%|████████████████████████████| 7/7 [00:02<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: val mIoU = 0.1414\n",
      ">> Saved new best to semnav_terrain/runs/fold0/best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [train]: 100%|████████████| 33/33 [00:38<00:00,  1.15s/it, loss=0.311]\n",
      "Epoch 5/5 [val]: 100%|████████████████████████████| 7/7 [00:02<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: val mIoU = 0.1426\n",
      ">> Saved new best to semnav_terrain/runs/fold0/best.pth\n",
      "Training done. Best mIoU: 0.14264614880084991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Implements a simple training loop (CPU) for fold=0.\n",
    "# - Tracks val mIoU and saves best.pth into runs/fold0/.\n",
    "# - Small epochs & batch size to validate the pipeline.\n",
    "\n",
    "import os, time, math\n",
    "import torch, yaml\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from semnav.models.deeplabv3 import DeeplabV3_MBV3\n",
    "from semnav.losses.dice import DiceLoss\n",
    "from semnav.losses.boundary import BoundaryLoss\n",
    "from semnav.metrics.miou import compute_confusion_matrix, compute_miou_from_cm\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "cfg_path = root / \"configs\" / \"deeplabv3_mbv3.yaml\"\n",
    "with open(cfg_path, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "num_classes = int(cfg[\"num_classes\"])\n",
    "EPOCHS = int(cfg[\"train\"][\"epochs\"])\n",
    "BATCH_SIZE = int(cfg[\"train\"][\"batch_size\"])\n",
    "LR = float(cfg[\"optimizer\"][\"lr\"])\n",
    "WD = float(cfg[\"optimizer\"][\"weight_decay\"])\n",
    "use_amp = bool(cfg[\"train\"].get(\"amp\", False))\n",
    "\n",
    "device = torch.device(\"cpu\")  # CPU only as requested\n",
    "model = DeeplabV3_MBV3(num_classes=num_classes, pretrained=True).to(device)\n",
    "\n",
    "# DataLoaders from previous cell (train_loader, val_loader) are already set\n",
    "\n",
    "# Class weights (median frequency) optional -> here just None (beginner-friendly)\n",
    "ce_weight = None\n",
    "ce = torch.nn.CrossEntropyLoss(ignore_index=255, weight=ce_weight)\n",
    "dice = DiceLoss(ignore_index=255)\n",
    "bnd  = BoundaryLoss(ignore_index=255, weight=1.0)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "run_dir = root / \"runs\" / \"fold0\"\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_miou = -1.0\n",
    "best_path = run_dir / \"best.pth\"\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [train]\")\n",
    "    for xb, db, yb, kb in pbar:\n",
    "    # skip tiny batch to avoid BN error in ASPP pooling\n",
    "        if xb.size(0) == 1:\n",
    "            continue\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss_ce = ce(logits, yb)\n",
    "        loss_dice = dice(logits, yb, num_classes=num_classes) * float(cfg[\"loss\"][\"dice\"])\n",
    "        loss_bnd  = bnd(logits, yb) * float(cfg[\"loss\"][\"boundary\"])\n",
    "        loss = loss_ce + loss_dice + loss_bnd\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix(loss=np.mean(losses))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    cm = torch.zeros((num_classes, num_classes), dtype=torch.int64, device=device)\n",
    "    with torch.no_grad():\n",
    "        for xb, db, yb, kb in tqdm(val_loader, desc=f\"Epoch {epoch}/{EPOCHS} [val]\"):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            cm += compute_confusion_matrix(pred, yb, num_classes=num_classes, ignore_index=255)\n",
    "    miou, per_class = compute_miou_from_cm(cm)\n",
    "    print(f\"Epoch {epoch}: val mIoU = {miou:.4f}\")\n",
    "\n",
    "    if miou > best_miou:\n",
    "        best_miou = miou\n",
    "        torch.save({\"model\": model.state_dict(), \"miou\": best_miou, \"epoch\": epoch}, best_path)\n",
    "        print(\">> Saved new best to\", best_path)\n",
    "\n",
    "print(\"Training done. Best mIoU:\", best_miou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0dda3e7-9794-449b-9df7-2cfef392ab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: 0 Unexpected keys: 8\n",
      "Running qualitative predictions on 12 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 12/12 [00:02<00:00,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved overlays to: semnav_terrain/runs/fold0/vis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Loads best.pth (CPU), ignoring unexpected 'aux_classifier' keys.\n",
    "# - Runs the model on a handful of val images.\n",
    "# - Saves color overlays to runs/fold0/vis/.\n",
    "\n",
    "import sys, json\n",
    "import cv2, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure local package is importable (adjust if needed)\n",
    "project_root = Path(\"semnav_terrain\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from semnav.models.deeplabv3 import DeeplabV3_MBV3  # after sys.path fix\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "vis_dir = root / \"runs\" / \"fold0\" / \"vis\"\n",
    "vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Palette in RGB (we'll convert to BGR for OpenCV overlays)\n",
    "PALETTE_RGB = np.array([\n",
    "  [128,64,128],  # ground\n",
    "  [244,35,232],  # sidewalk\n",
    "  [70,70,70],    # stairs\n",
    "  [0,0,142],     # water\n",
    "  [220,20,60],   # person\n",
    "  [0,0,230],     # car\n",
    "  [70,130,180],  # sky\n",
    "], dtype=np.uint8)\n",
    "PALETTE_BGR = PALETTE_RGB[:, ::-1]  # convert to BGR for cv2\n",
    "\n",
    "# Load trained weights (CPU) and ignore unexpected keys like aux_classifier.*\n",
    "ckpt_path = root / \"runs\" / \"fold0\" / \"best.pth\"\n",
    "assert ckpt_path.exists(), f\"Checkpoint not found: {ckpt_path}\"\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "model = DeeplabV3_MBV3(num_classes=7, pretrained=False)\n",
    "incompat = model.load_state_dict(ckpt[\"model\"], strict=False)  # <-- key fix\n",
    "# Optional: print a small summary of missing/unexpected (PyTorch returns IncompatibleKeys)\n",
    "try:\n",
    "    print(\"Missing keys:\", len(incompat.missing_keys), \"Unexpected keys:\", len(incompat.unexpected_keys))\n",
    "except Exception:\n",
    "    pass\n",
    "model.eval()\n",
    "\n",
    "# Helper: preprocessing like in training (normalize to ImageNet stats)\n",
    "def preprocess_to_tensor(bgr, w, h):\n",
    "    resized = cv2.resize(bgr, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    rgb = (rgb - mean) / std\n",
    "    x = torch.from_numpy(rgb.transpose(2, 0, 1)).unsqueeze(0)  # [1,3,H,W]\n",
    "    return x\n",
    "\n",
    "# Load a few val keys\n",
    "splits_dir = root / \"data\" / \"splits\"\n",
    "val_keys = [l.strip() for l in open(splits_dir / \"fold0_val.txt\") if l.strip()]\n",
    "val_keys = val_keys[:12] if len(val_keys) > 12 else val_keys\n",
    "\n",
    "img_dir = root / \"data\" / \"images\"\n",
    "lbl_dir = root / \"data\" / \"labels\"\n",
    "\n",
    "# Input size (W,H)\n",
    "cfg_inp = (512, 384)\n",
    "\n",
    "print(f\"Running qualitative predictions on {len(val_keys)} images...\")\n",
    "for key in tqdm(val_keys):\n",
    "    img_path = img_dir / f\"{key}.png\"\n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    bgr = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "    if bgr is None:\n",
    "        continue\n",
    "    H, W = bgr.shape[:2]\n",
    "\n",
    "    x = preprocess_to_tensor(bgr, *cfg_inp)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)              # [1,C,h,w]\n",
    "        pred = torch.argmax(logits, dim=1)[0].cpu().numpy().astype(np.uint8)  # [h,w]\n",
    "\n",
    "    # Colorize and resize back to original size\n",
    "    vis_small = PALETTE_BGR[pred]  # [h,w,3] in BGR\n",
    "    vis = cv2.resize(vis_small, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Blend overlay\n",
    "    overlay = cv2.addWeighted(bgr, 0.45, vis, 0.55, 0.0)\n",
    "\n",
    "    # Optional: highlight free-space (ground ∪ sidewalk => ids 0 and 1)\n",
    "    free_small = ((pred == 0) | (pred == 1)).astype(np.uint8) * 255\n",
    "    free = cv2.resize(free_small, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "    overlay_free = overlay.copy()\n",
    "    overlay_free[free == 255] = (0.8 * overlay_free[free == 255] + np.array([0, 255, 0]) * 0.2).astype(np.uint8)\n",
    "\n",
    "    # Save\n",
    "    out_path = vis_dir / f\"{key}_overlay.png\"\n",
    "    cv2.imwrite(str(out_path), overlay_free)\n",
    "\n",
    "print(f\"Saved overlays to: {vis_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbad5d79-617c-41d1-9d42-cb0f05472a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote FusionLite model stub. (Training optional on CPU)\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Writes a lightweight FusionLite model file that fuses a depth branch into RGB features.\n",
    "# - This is optional for CPU; you can skip training this model for now.\n",
    "\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "fusion_code = textwrap.dedent(r\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
    "\n",
    "class DepthBranch(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 32, 3, 2, 1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, out_ch, 3, 2, 1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):  # downsample depth to stride ~8\n",
    "        return self.enc(x)\n",
    "\n",
    "class FusionLite(nn.Module):\n",
    "    def __init__(self, num_classes=7, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.rgb = deeplabv3_mobilenet_v3_large(weights='DEFAULT' if pretrained else None)\n",
    "        # head in_ch\n",
    "        in_ch = self.rgb.classifier[-1].in_channels\n",
    "        self.rgb.classifier[-1] = nn.Conv2d(in_ch, num_classes, 1)\n",
    "\n",
    "        self.depth = DepthBranch(1, out_ch=128)\n",
    "        # gating conv for fusion at stride ~8\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_ch + 128, in_ch, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_rgb, x_depth):\n",
    "        # feature from backbone classifier input (ASPP in-channels)\n",
    "        # In torchvision impl, classifier input is not directly exposed;\n",
    "        # simple trick: run forward once to get logits, then re-run limited layers is complex.\n",
    "        # Here we do late fusion into logits space (approx): fuse via gate at classifier input proxy using ASPP output.\n",
    "        out_dict = self.rgb(x_rgb)\n",
    "        logits_rgb = out_dict['out']\n",
    "\n",
    "        # make depth features stride ~8\n",
    "        Fd = self.depth(x_depth)\n",
    "        # up/down sample to match logits spatial size\n",
    "        if Fd.shape[-2:] != logits_rgb.shape[-2:]:\n",
    "            Fd = F.interpolate(Fd, size=logits_rgb.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # build a proxy feature stack: [logits_rgb (C classes) , Fd(128)]\n",
    "        # expand logits to in_ch by 1x1 conv to match channels\n",
    "        proj = nn.Conv2d(logits_rgb.shape[1], logits_rgb.shape[1], 1).to(logits_rgb.device)\n",
    "        Frgb_proj = proj(logits_rgb)\n",
    "        gate_in = torch.cat([Frgb_proj, Fd], dim=1)\n",
    "        g = self.gate(gate_in)\n",
    "        fused = Frgb_proj + g * Fd.mean(dim=1, keepdim=True)  # simple gate on avg depth feat\n",
    "        # final 1x1 to classes\n",
    "        head = nn.Conv2d(fused.shape[1], logits_rgb.shape[1], 1).to(logits_rgb.device)\n",
    "        out = head(fused)\n",
    "        return out\n",
    "\"\"\")\n",
    "\n",
    "(root / \"semnav\" / \"models\" / \"fusion_lite.py\").write_text(fusion_code)\n",
    "print(\"Wrote FusionLite model stub. (Training optional on CPU)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b180035-6304-40b8-9675-8190fc14e3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val mIoU: 0.1426  |  Boundary IoU: 0.0\n",
      "Wrote CSV -> semnav_terrain/runs/fold0/val_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 5A (fixed v2) — evaluate & log CSV with aux-keys ignored and proper tensor inputs\n",
    "\n",
    "import csv, torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from semnav.models.deeplabv3 import DeeplabV3_MBV3\n",
    "from semnav.metrics.miou import compute_confusion_matrix, compute_miou_from_cm\n",
    "try:\n",
    "    from semnav.metrics.boundary_iou import boundary_iou_free\n",
    "    HAS_BIOU = True\n",
    "except Exception:\n",
    "    HAS_BIOU = False\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "ckpt_path = root / \"runs\" / \"fold0\" / \"best.pth\"\n",
    "assert ckpt_path.exists(), f\"Missing checkpoint: {ckpt_path}\"\n",
    "\n",
    "# Load model (CPU), ignore aux head keys\n",
    "model = DeeplabV3_MBV3(num_classes=7, pretrained=False)\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "_ = model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "def evaluate_to_csv(model, loader, csv_path, num_classes=7):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device).eval()\n",
    "\n",
    "    cm = torch.zeros((num_classes, num_classes), dtype=torch.int64, device=device)\n",
    "    biou_vals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, db, yb, kb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            pred = torch.argmax(logits, dim=1)  # [B,H,W] (torch.LongTensor)\n",
    "\n",
    "            cm += compute_confusion_matrix(pred, yb, num_classes=num_classes, ignore_index=255)\n",
    "\n",
    "            if HAS_BIOU:\n",
    "                # >>> PASS TORCH TENSORS (CPU) — not numpy <<<\n",
    "                for i in range(pred.size(0)):\n",
    "                    biou_vals.append(boundary_iou_free(pred[i].cpu(), yb[i].cpu()))\n",
    "\n",
    "    miou, per_class = compute_miou_from_cm(cm)\n",
    "    biou = float(np.nanmean(biou_vals)) if (HAS_BIOU and len(biou_vals) > 0) else np.nan\n",
    "\n",
    "    # write CSV row\n",
    "    csv_path = Path(csv_path)\n",
    "    write_header = not csv_path.exists()\n",
    "    with csv_path.open(\"a\", newline=\"\") as fp:\n",
    "        w = csv.writer(fp)\n",
    "        if write_header:\n",
    "            w.writerow([\"miou\"] + [f\"class_{i}_iou\" for i in range(num_classes)] + [\"boundary_iou\"])\n",
    "        w.writerow([float(miou)] + [float(x) for x in per_class] + [biou])\n",
    "\n",
    "    return miou, biou\n",
    "\n",
    "miou, biou = evaluate_to_csv(model, val_loader, root / \"runs\" / \"fold0\" / \"val_metrics.csv\", num_classes=7)\n",
    "print(f\"Val mIoU: {miou:.4f}  |  Boundary IoU: {biou if not np.isnan(biou) else 'N/A'}\")\n",
    "print(\"Wrote CSV ->\", root / \"runs\" / \"fold0\" / \"val_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7155a096-c384-4112-b4e4-b70166661b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote summary CSV -> semnav_terrain/reports/summary.csv\n",
      "Saved figure -> semnav_terrain/reports/figs/metrics_headline.png\n",
      "Saved figure -> semnav_terrain/reports/figs/per_class_iou.png\n",
      "Done. Open reports/summary.csv and reports/figs/*.png to view results.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5B — Fix metrics summary (read multi-column CSV, write summary.csv, save charts)\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "metrics_csv = root / \"runs\" / \"fold0\" / \"val_metrics.csv\"\n",
    "reports_dir = root / \"reports\"\n",
    "figs_dir = reports_dir / \"figs\"\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "figs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Read the multi-column CSV and take the LAST row\n",
    "assert metrics_csv.exists(), f\"Metrics CSV not found: {metrics_csv}\"\n",
    "rows = []\n",
    "with metrics_csv.open(\"r\", newline=\"\") as f:\n",
    "    r = csv.DictReader(f)\n",
    "    for row in r:\n",
    "        rows.append(row)\n",
    "assert rows, f\"No rows found in {metrics_csv}\"\n",
    "last = rows[-1]  # latest metrics\n",
    "\n",
    "# 2) Parse numeric fields safely\n",
    "def to_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "miou = to_float(last.get(\"miou\", \"nan\"))\n",
    "biou = to_float(last.get(\"boundary_iou\", \"nan\"))\n",
    "\n",
    "# collect per-class ious (any headers like class_0_iou ... class_6_iou)\n",
    "class_cols = [k for k in last.keys() if k.startswith(\"class_\") and k.endswith(\"_iou\")]\n",
    "class_cols_sorted = sorted(\n",
    "    class_cols,\n",
    "    key=lambda k: int(k.split(\"_\")[1]) if k.split(\"_\")[1].isdigit() else 999\n",
    ")\n",
    "per_class_vals = [to_float(last[k]) for k in class_cols_sorted]\n",
    "\n",
    "# 3) Write a simple 2-column summary CSV: metric,value (includes per-class + headline metrics)\n",
    "summary_csv = reports_dir / \"summary.csv\"\n",
    "with summary_csv.open(\"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"metric\", \"value\"])\n",
    "    w.writerow([\"miou\", miou])\n",
    "    w.writerow([\"boundary_iou\", biou])\n",
    "    for k, v in zip(class_cols_sorted, per_class_vals):\n",
    "        w.writerow([k, v])\n",
    "\n",
    "print(\"Wrote summary CSV ->\", summary_csv)\n",
    "\n",
    "# 4) Save charts (matplotlib, single-color default)\n",
    "# 4a) Headline metrics bar (mIoU & Boundary IoU)\n",
    "labels = [\"mIoU\", \"Boundary IoU\"]\n",
    "vals = [miou if not np.isnan(miou) else 0.0,\n",
    "        biou if not np.isnan(biou) else 0.0]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels, vals)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.title(\"Validation Metrics (fold0 latest)\")\n",
    "plt.ylabel(\"Score\")\n",
    "for i, v in enumerate(vals):\n",
    "    txt = f\"{v:.3f}\" if not np.isnan([miou, biou][i]) else \"N/A\"\n",
    "    plt.text(i, min(v + 0.02, 0.98), txt, ha=\"center\", va=\"bottom\")\n",
    "head_fig = figs_dir / \"metrics_headline.png\"\n",
    "plt.savefig(head_fig, dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved figure ->\", head_fig)\n",
    "\n",
    "# 4b) Per-class IoU bar chart (if available)\n",
    "if per_class_vals:\n",
    "    plt.figure()\n",
    "    x = np.arange(len(per_class_vals))\n",
    "    plt.bar(x, per_class_vals)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.title(\"Per-class IoU (fold0 latest)\")\n",
    "    plt.ylabel(\"IoU\")\n",
    "    plt.xlabel(\"Class index\")\n",
    "    plt.xticks(x, [c.split(\"_\")[1] for c in class_cols_sorted])\n",
    "    for i, v in enumerate(per_class_vals):\n",
    "        plt.text(i, min(v + 0.02, 0.98), f\"{v:.2f}\" if not np.isnan(v) else \"N/A\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "    cls_fig = figs_dir / \"per_class_iou.png\"\n",
    "    plt.savefig(cls_fig, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved figure ->\", cls_fig)\n",
    "else:\n",
    "    print(\"No per-class IoU columns found in CSV; skipped per-class chart.\")\n",
    "\n",
    "print(\"Done. Open reports/summary.csv and reports/figs/*.png to view results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "880ad4dc-624c-488d-9578-e7316fc487be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TorchScript -> semnav_terrain/export/model.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/7h574p2n2pg8r4mrdg5lhzxc0000gn/T/ipykernel_26620/3950683732.py:44: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(model, dummy, str(onnx_path), dynamo=False, **export_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ONNX -> semnav_terrain/export/model.onnx\n",
      "Sanity forward OK. Logits shape: (1, 7, 384, 512)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6A-legacy — Export TorchScript + ONNX using the legacy exporter (no onnxscript required)\n",
    "\n",
    "import sys, torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Make local package importable\n",
    "project_root = Path(\"semnav_terrain\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from semnav.models.deeplabv3 import DeeplabV3_MBV3\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "ckpt_path = root / \"runs\" / \"fold0\" / \"best.pth\"\n",
    "export_dir = root / \"export\"\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Build model (CPU) and load weights, stripping aux head keys\n",
    "model = DeeplabV3_MBV3(num_classes=7, pretrained=False)\n",
    "state = torch.load(ckpt_path, map_location=\"cpu\")[\"model\"]\n",
    "# Strip any aux head weights so strict load succeeds\n",
    "state = {k: v for k, v in state.items() if not k.startswith(\"net.aux_classifier.\")}\n",
    "_ = model.load_state_dict(state, strict=False)\n",
    "model.eval()\n",
    "\n",
    "# 2) TorchScript (trace)\n",
    "dummy = torch.randn(1, 3, 384, 512)  # H,W as in config\n",
    "ts_path = export_dir / \"model.ts\"\n",
    "traced = torch.jit.trace(model, dummy)\n",
    "traced.save(str(ts_path))\n",
    "print(\"Saved TorchScript ->\", ts_path)\n",
    "\n",
    "# 3) ONNX export (force legacy exporter by passing dynamo=False; fallback if arg not supported)\n",
    "onnx_path = export_dir / \"model.onnx\"\n",
    "export_kwargs = dict(\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\"input\": {0: \"N\", 2: \"H\", 3: \"W\"},\n",
    "                  \"logits\": {0: \"N\", 2: \"h\", 3: \"w\"}},\n",
    "    opset_version=12,\n",
    "    do_constant_folding=True,\n",
    ")\n",
    "try:\n",
    "    torch.onnx.export(model, dummy, str(onnx_path), dynamo=False, **export_kwargs)\n",
    "except TypeError:\n",
    "    # Older torch without 'dynamo' arg\n",
    "    torch.onnx.export(model, dummy, str(onnx_path), **export_kwargs)\n",
    "\n",
    "print(\"Saved ONNX ->\", onnx_path)\n",
    "\n",
    "# Quick forward check\n",
    "with torch.no_grad():\n",
    "    out = model(dummy)\n",
    "    print(\"Sanity forward OK. Logits shape:\", tuple(out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "009f91d3-bb91-478f-aa88-99ba3892f81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compared on 10 images.\n",
      "Mean pixel disagreement: 0.0\n",
      "Max pixel disagreement: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 6B — PyTorch ↔ ONNX parity check (strips aux keys, CPU)\n",
    "\n",
    "import sys, cv2, numpy as np, torch, onnxruntime as ort\n",
    "from pathlib import Path\n",
    "\n",
    "# Make local package importable\n",
    "project_root = Path(\"semnav_terrain\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from semnav.models.deeplabv3 import DeeplabV3_MBV3\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "ckpt_path = root / \"runs\" / \"fold0\" / \"best.pth\"\n",
    "onnx_path = root / \"export\" / \"model.onnx\"\n",
    "\n",
    "# --- Build PT model (CPU) and load weights, STRIPPING aux head keys ---\n",
    "state = torch.load(ckpt_path, map_location=\"cpu\")[\"model\"]\n",
    "state = {k: v for k, v in state.items() if not k.startswith(\"net.aux_classifier.\")}\n",
    "pt_model = DeeplabV3_MBV3(num_classes=7, pretrained=False)\n",
    "_ = pt_model.load_state_dict(state, strict=True)\n",
    "pt_model.eval()\n",
    "\n",
    "# --- ONNXRuntime session (CPU) ---\n",
    "sess = ort.InferenceSession(str(onnx_path), providers=[\"CPUExecutionProvider\"])\n",
    "inp_name = sess.get_inputs()[0].name\n",
    "\n",
    "# --- Preprocess: BGR -> RGB, normalize, NCHW float32 @ 512x384 ---\n",
    "mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "W, H = 512, 384\n",
    "\n",
    "def preprocess(bgr):\n",
    "    rsz = cv2.resize(bgr, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "    rgb = cv2.cvtColor(rsz, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    rgb = (rgb - mean) / std\n",
    "    x = rgb.transpose(2, 0, 1)[None, ...].astype(np.float32)  # [1,3,H,W]\n",
    "    return x\n",
    "\n",
    "# --- Compare on a handful of val images ---\n",
    "val_keys = [l.strip() for l in open(root/\"data\"/\"splits\"/\"fold0_val.txt\") if l.strip()]\n",
    "val_keys = val_keys[:10] if len(val_keys) > 10 else val_keys\n",
    "\n",
    "diffs = []\n",
    "for key in val_keys:\n",
    "    img = cv2.imread(str(root / \"data\" / \"images\" / f\"{key}.png\"), cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        continue\n",
    "    x_np = preprocess(img)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_pt = pt_model(torch.from_numpy(x_np).float())\n",
    "        pm_pt = logits_pt.argmax(1).cpu().numpy()    # [1,h,w]\n",
    "\n",
    "    logits_onnx = sess.run(None, {inp_name: x_np})[0]  # [1,C,h,w]\n",
    "    pm_onnx = logits_onnx.argmax(1)                    # [1,h,w]\n",
    "\n",
    "    # Ensure shapes match; if not, resize ONNX pred to PT shape\n",
    "    if pm_pt.shape != pm_onnx.shape:\n",
    "        h, w = pm_pt.shape[-2], pm_pt.shape[-1]\n",
    "        pm_onnx = pm_onnx.astype(np.uint8)\n",
    "        pm_onnx = np.array([cv2.resize(pm_onnx[0], (w, h), interpolation=cv2.INTER_NEAREST)])[None, ...]\n",
    "\n",
    "    diffs.append(np.mean(pm_pt != pm_onnx))\n",
    "\n",
    "print(\"Compared on\", len(diffs), \"images.\")\n",
    "print(\"Mean pixel disagreement:\", float(np.mean(diffs)) if diffs else \"N/A\")\n",
    "print(\"Max pixel disagreement:\", float(np.max(diffs)) if diffs else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8b47985-8da8-4141-805a-262c9c43d7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paths]\n",
      "  weights     : /Users/jayamdaxeshkumarshah/semnav_terrain/export/model.onnx\n",
      "  pseudo-depth: /Users/jayamdaxeshkumarshah/semnav_terrain/models/midas_small.onnx\n",
      "  det-onnx    : /Users/jayamdaxeshkumarshah/semnav_terrain/models/yolov5n.onnx\n",
      "Webcam demo finished.\n"
     ]
    }
   ],
   "source": [
    "# webcam_demo.py — segmentation + free-space + pseudo-depth + YOLO boxes within X meters\n",
    "# Keys:\n",
    "#   ESC/q : quit\n",
    "#   c     : calibrate depth (maps center ROI to --calib-distance meters)\n",
    "\n",
    "import argparse, time, subprocess, platform\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# ---------------- path helpers (robust for VS Code & Jupyter) ----------------\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # __file__ doesn't exist in a Jupyter cell → use current working directory\n",
    "    SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "def resolve_path(plike):\n",
    "    \"\"\"Expand ~, env vars; if relative, try as-is, SCRIPT_DIR/..., and CWD/...\"\"\"\n",
    "    if plike is None:\n",
    "        return None\n",
    "    p = Path(str(plike)).expanduser()\n",
    "    cands = [p]\n",
    "    if not p.is_absolute():\n",
    "        cands += [SCRIPT_DIR / p, Path.cwd() / p]\n",
    "    for c in cands:\n",
    "        if c.exists():\n",
    "            return c\n",
    "    # not found; return first candidate so error message shows what we tried first\n",
    "    return cands[0]\n",
    "\n",
    "# ---------------- optional deps ----------------\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_OK = True\n",
    "except Exception:\n",
    "    TORCH_OK = False\n",
    "\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "    ORT_OK = True\n",
    "except Exception:\n",
    "    ORT_OK = False\n",
    "\n",
    "# ---------------- palettes / labels ----------------\n",
    "PALETTE = np.array([\n",
    "  [128,  64, 128],  # ground\n",
    "  [244,  35, 232],  # sidewalk\n",
    "  [ 70,  70,  70],  # stairs\n",
    "  [  0,   0, 142],  # water\n",
    "  [220,  20,  60],  # person\n",
    "  [  0,   0, 230],  # car\n",
    "  [ 70, 130, 180],  # sky\n",
    "], dtype=np.uint8)\n",
    "\n",
    "COCO80 = [\n",
    "    'person','bicycle','car','motorcycle','airplane','bus','train','truck','boat','traffic light',\n",
    "    'fire hydrant','stop sign','parking meter','bench','bird','cat','dog','horse','sheep','cow',\n",
    "    'elephant','bear','zebra','giraffe','backpack','umbrella','handbag','tie','suitcase','frisbee',\n",
    "    'skis','snowboard','sports ball','kite','baseball bat','baseball glove','skateboard',\n",
    "    'surfboard','tennis racket','bottle','wine glass','cup','fork','knife','spoon','bowl','banana',\n",
    "    'apple','sandwich','orange','broccoli','carrot','hot dog','pizza','donut','cake','chair','couch',\n",
    "    'potted plant','bed','dining table','toilet','tv','laptop','mouse','remote','keyboard','cell phone',\n",
    "    'microwave','oven','toaster','sink','refrigerator','book','clock','vase','scissors','teddy bear',\n",
    "    'hair drier','toothbrush'\n",
    "]\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def preprocess(frame_bgr, w, h):\n",
    "    img = cv2.resize(frame_bgr, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    x = img.astype(np.float32) / 255.0\n",
    "    x = x[:, :, ::-1]\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    x = (x - mean) / std\n",
    "    x = x.transpose(2, 0, 1)[None, ...].astype(np.float32, copy=False)\n",
    "    return x\n",
    "\n",
    "def preprocess_depth(frame_bgr, w, h):\n",
    "    img = cv2.resize(frame_bgr, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    x = img.astype(np.float32) / 255.0\n",
    "    x = x[:, :, ::-1]\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    x = (x - mean) / std\n",
    "    x = x.transpose(2, 0, 1)[None, ...].astype(np.float32, copy=False)\n",
    "    return x\n",
    "\n",
    "def colorize(mask):\n",
    "    return PALETTE[mask]\n",
    "\n",
    "def speak(text, say_cmd=\"say\", enable=True):\n",
    "    if not enable or not text:\n",
    "        return False\n",
    "    if platform.system() == \"Darwin\":\n",
    "        try:\n",
    "            subprocess.Popen([say_cmd, text])\n",
    "        except Exception:\n",
    "            print(\"[speak]\", text, flush=True)\n",
    "    else:\n",
    "        print(\"[speak]\", text, flush=True)\n",
    "    return True\n",
    "\n",
    "# ---------------- YOLO utils ----------------\n",
    "def letterbox(im, new_shape=640, color=(114,114,114), stride=32):\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "    shape = im.shape[:2]\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    new_unpad = (int(round(shape[1] * r)), int(round(shape[0] * r)))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]\n",
    "    dw /= 2; dh /= 2\n",
    "    if shape[::-1] != new_unpad:\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "    return im, r, (left, top)\n",
    "\n",
    "def yolo_preprocess_bgr(im, size=640):\n",
    "    im0 = im.copy()\n",
    "    img, ratio, pad = letterbox(im0, new_shape=size, stride=32)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    img = img.transpose(2, 0, 1)[None, ...]\n",
    "    return img.astype(np.float32), ratio, pad, im0.shape[:2]\n",
    "\n",
    "def nms_boxes(boxes, scores, iou_thres=0.45):\n",
    "    if len(boxes) == 0: return []\n",
    "    boxes = boxes.astype(np.float32)\n",
    "    x1, y1, x2, y2 = boxes.T\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    order = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]; keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "        w = np.maximum(0.0, xx2 - xx1)\n",
    "        h = np.maximum(0.0, yy2 - yy1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)\n",
    "        inds = np.where(ovr <= iou_thres)[0]\n",
    "        order = order[inds + 1]\n",
    "    return keep\n",
    "\n",
    "def yolo_decode(pred, ratio, pad, orig_hw, conf_thres=0.35, iou_thres=0.45, classes=None, max_det=100):\n",
    "    p = np.squeeze(pred, axis=0)\n",
    "    if p.shape[1] < 85: return []\n",
    "    xywh = p[:, 0:4]; obj = p[:, 4:5]; cls = p[:, 5:]\n",
    "    cls_id = np.argmax(cls, axis=1)\n",
    "    cls_score = cls[np.arange(cls.shape[0]), cls_id]\n",
    "    scores = (obj[:, 0] * cls_score)\n",
    "    mask = scores >= conf_thres\n",
    "    if classes is not None:\n",
    "        mask = mask & np.isin(cls_id, classes)\n",
    "    xywh = xywh[mask]; scores = scores[mask]; cls_id = cls_id[mask]\n",
    "    if xywh.size == 0: return []\n",
    "    x, y, w, h = xywh.T\n",
    "    x1 = x - w/2; y1 = y - h/2; x2 = x + w/2; y2 = y + h/2\n",
    "    gain = ratio; padx, pady = pad\n",
    "    x1 = (x1 - padx) / gain; y1 = (y1 - pady) / gain\n",
    "    x2 = (x2 - padx) / gain; y2 = (y2 - pady) / gain\n",
    "    H, W = orig_hw\n",
    "    x1 = np.clip(x1, 0, W-1); y1 = np.clip(y1, 0, H-1)\n",
    "    x2 = np.clip(x2, 0, W-1); y2 = np.clip(y2, 0, H-1)\n",
    "    boxes = np.vstack([x1, y1, x2, y2]).T\n",
    "    keep = nms_boxes(boxes, scores, iou_thres=iou_thres)\n",
    "    if len(keep) > max_det: keep = keep[:max_det]\n",
    "    return [(boxes[i], float(scores[i]), int(cls_id[i])) for i in keep]\n",
    "\n",
    "# ---------------- IoU + simple track match ----------------\n",
    "def iou_xyxy(a, b):\n",
    "    ax1, ay1, ax2, ay2 = a; bx1, by1, bx2, by2 = b\n",
    "    ix1, iy1 = max(ax1, bx1), max(ay1, by1)\n",
    "    ix2, iy2 = min(ax2, bx2), min(ay2, by2)\n",
    "    iw, ih = max(0, ix2 - ix1), max(0, iy2 - iy1)\n",
    "    inter = iw * ih\n",
    "    if inter == 0: return 0.0\n",
    "    aarea = (ax2 - ax1) * (ay2 - ay1)\n",
    "    barea = (bx2 - bx1) * (by2 - by1)\n",
    "    return inter / (aarea + barea - inter + 1e-6)\n",
    "\n",
    "def match_track_iou(tracks, name, box, iou_thresh=0.3):\n",
    "    best_k, best_iou = None, iou_thresh\n",
    "    for k, t in tracks.items():\n",
    "        if t[\"name\"] != name: continue\n",
    "        iou = iou_xyxy(t[\"box\"], box)\n",
    "        if iou > best_iou:\n",
    "            best_iou, best_k = iou, k\n",
    "    return best_k\n",
    "\n",
    "# ---------------- main ----------------\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    # segmentation (defaults relative to this file)\n",
    "    ap.add_argument('--weights', default=str(SCRIPT_DIR / 'export/model.onnx'))\n",
    "    ap.add_argument('--backend', choices=['onnx','torch'], default='onnx')\n",
    "    ap.add_argument('--input-size', nargs=2, type=int, default=[512,384], metavar=('W','H'))\n",
    "    ap.add_argument('--classes', nargs='+', default=['ground','sidewalk','stairs','water','person','car','sky'])\n",
    "    ap.add_argument('--free-space', nargs='+', default=['ground','sidewalk'])\n",
    "    # display\n",
    "    ap.add_argument('--show-fps', action='store_true')\n",
    "    ap.add_argument('--save-video', default=None)\n",
    "    ap.add_argument('--alpha', type=float, default=0.55)\n",
    "    ap.add_argument('--free-alpha', type=float, default=0.20)\n",
    "    ap.add_argument('--morph', type=int, default=0)\n",
    "    # speech + anti-chatter\n",
    "    ap.add_argument('--speak', action='store_true')\n",
    "    ap.add_argument('--say-cmd', default='say')\n",
    "    ap.add_argument('--cooldown', type=float, default=6.0)\n",
    "    ap.add_argument('--max-announce', type=int, default=2)\n",
    "    ap.add_argument('--speak-classes', nargs='+', default=['water','stairs','person','car'])\n",
    "    ap.add_argument('--speak-once', action='store_true')\n",
    "    ap.add_argument('--speak-miss-frames', type=int, default=90)\n",
    "    ap.add_argument('--speak-min-gap', type=float, default=2.5)\n",
    "    ap.add_argument('--speak-per-class-gap', type=float, default=12.0)\n",
    "    ap.add_argument('--reannounce-drop', type=float, default=0.6)\n",
    "    ap.add_argument('--reannounce-abs', type=float, default=1.5)\n",
    "    ap.add_argument('--reannounce-time', type=float, default=12.0)\n",
    "    # pseudo-depth (default relative to this file)\n",
    "    ap.add_argument('--pseudo-depth', default=str(SCRIPT_DIR / 'models/midas_small.onnx'))\n",
    "    ap.add_argument('--depth-size', nargs=2, type=int, default=[256,256], metavar=('W','H'))\n",
    "    ap.add_argument('--depth-interval', type=int, default=2)\n",
    "    ap.add_argument('--depth-ema', type=float, default=0.7)\n",
    "    ap.add_argument('--calib-distance', type=float, default=2.0)\n",
    "    # detection (YOLOv5n ONNX, default relative to this file)\n",
    "    ap.add_argument('--det-onnx', default=str(SCRIPT_DIR / 'models/yolov5n.onnx'))\n",
    "    ap.add_argument('--det-size', type=int, default=640)\n",
    "    ap.add_argument('--det-interval', type=int, default=2)\n",
    "    ap.add_argument('--det-thresh', type=float, default=0.35)\n",
    "    ap.add_argument('--det-iou', type=float, default=0.45)\n",
    "    ap.add_argument('--det-classes', nargs='*', default=None)\n",
    "    # filtering\n",
    "    ap.add_argument('--max-distance', type=float, default=5.0)\n",
    "    ap.add_argument('--min-area', type=int, default=120)\n",
    "    ap.add_argument('--bottom-focus', type=float, default=0.0)\n",
    "\n",
    "    # Jupyter-safe parsing\n",
    "    args, _ = ap.parse_known_args()\n",
    "\n",
    "    # -------- resolve paths and print them (helps Notebook users) --------\n",
    "    weights_path = resolve_path(args.weights)\n",
    "    depth_path   = resolve_path(args.pseudo_depth) if args.pseudo_depth else None\n",
    "    det_path     = resolve_path(args.det_onnx) if args.det_onnx else None\n",
    "\n",
    "    print(\"[paths]\")\n",
    "    print(\"  weights     :\", weights_path)\n",
    "    print(\"  pseudo-depth:\", depth_path if depth_path else \"None\")\n",
    "    print(\"  det-onnx    :\", det_path if det_path else \"None\", flush=True)\n",
    "\n",
    "    # sanity checks\n",
    "    if not weights_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Segmentation weights not found at '{args.weights}'. \"\n",
    "            f\"Looked for: {weights_path}. \"\n",
    "            f\"Tip: run from your project root or pass an absolute --weights path.\"\n",
    "        )\n",
    "\n",
    "    w, h = args.input_size\n",
    "    name_to_id = {n:i for i,n in enumerate(args.classes)}\n",
    "    fs_ids = [name_to_id[c] for c in args.free_space if c in name_to_id]\n",
    "\n",
    "    # seg init\n",
    "    if args.backend == 'onnx':\n",
    "        assert ORT_OK, \"onnxruntime not available.\"\n",
    "        seg_sess = ort.InferenceSession(str(weights_path), providers=['CPUExecutionProvider'])\n",
    "        seg_in_name = seg_sess.get_inputs()[0].name\n",
    "        seg_model = None\n",
    "    else:\n",
    "        assert TORCH_OK, \"PyTorch not available.\"\n",
    "        if str(weights_path).endswith(\".ts\"):\n",
    "            seg_model = torch.jit.load(str(weights_path), map_location=\"cpu\").eval()\n",
    "        else:\n",
    "            from semnav.models.deeplabv3 import DeeplabV3_MBV3\n",
    "            state = torch.load(str(weights_path), map_location=\"cpu\")\n",
    "            if isinstance(state, dict) and \"model\" in state:\n",
    "                state = state[\"model\"]\n",
    "            state = {k:v for k,v in state.items() if \"aux_classifier\" not in k}\n",
    "            seg_model = DeeplabV3_MBV3(num_classes=len(args.classes), pretrained=False)\n",
    "            seg_model.load_state_dict(state, strict=False)\n",
    "            seg_model.eval()\n",
    "        seg_sess = None\n",
    "\n",
    "    # depth init\n",
    "    depth_sess = None; depth_in_name = None\n",
    "    depth_w, depth_h = args.depth_size\n",
    "    has_depth = False\n",
    "    if depth_path:\n",
    "        if depth_path.exists():\n",
    "            depth_sess = ort.InferenceSession(str(depth_path), providers=['CPUExecutionProvider'])\n",
    "            depth_in_name = depth_sess.get_inputs()[0].name\n",
    "            has_depth = True\n",
    "        else:\n",
    "            print(f\"[warn] pseudo-depth not found: {depth_path}\", flush=True)\n",
    "\n",
    "    # det init\n",
    "    det_sess = None; det_in_name = None\n",
    "    det_classes_idx = None\n",
    "    if det_path:\n",
    "        if det_path.exists():\n",
    "            det_sess = ort.InferenceSession(str(det_path), providers=['CPUExecutionProvider'])\n",
    "            det_in_name = det_sess.get_inputs()[0].name\n",
    "            if args.det_classes:\n",
    "                name2coco = {n:i for i,n in enumerate(COCO80)}\n",
    "                det_classes_idx = [name2coco[n] for n in args.det_classes if n in name2coco]\n",
    "        else:\n",
    "            print(f\"[warn] detector ONNX not found: {det_path}; continuing without boxes.\", flush=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Could not open webcam 0.\")\n",
    "\n",
    "    writer = None\n",
    "    if args.save_video:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        ret_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) or 640\n",
    "        ret_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) or 480\n",
    "        writer = cv2.VideoWriter(args.save_video, fourcc, 30.0, (ret_w, ret_h))\n",
    "\n",
    "    last_fps_t = time.time(); frames = 0\n",
    "    inv_depth_full = None; k_scale = None\n",
    "    last_dets = []\n",
    "    frame_idx = 0\n",
    "\n",
    "    # speech state\n",
    "    tracks = {}\n",
    "    next_track_id = 1\n",
    "    last_spoken_per_class = {}\n",
    "    last_any_spoken = 0.0\n",
    "\n",
    "    # morph for free space\n",
    "    if args.morph and args.morph > 0:\n",
    "        ksz = args.morph if (args.morph % 2 == 1) else (args.morph + 1)\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (ksz, ksz))\n",
    "    else:\n",
    "        kernel = None\n",
    "\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "\n",
    "        # seg\n",
    "        x = preprocess(frame, w, h)\n",
    "        if seg_sess is not None:\n",
    "            logits = seg_sess.run(None, {seg_in_name: x.astype(np.float32)})[0]\n",
    "            mask_small = logits.argmax(1)[0].astype(np.uint8)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                tx = torch.from_numpy(x)\n",
    "                out = seg_model(tx)\n",
    "                if isinstance(out, dict) and \"out\" in out:\n",
    "                    out = out[\"out\"]\n",
    "                mask_small = torch.argmax(out, dim=1)[0].cpu().numpy().astype(np.uint8)\n",
    "        mask = cv2.resize(mask_small, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # depth\n",
    "        if has_depth and (frame_idx % max(1, args.depth_interval) == 0):\n",
    "            xd = preprocess_depth(frame, depth_w, depth_h)\n",
    "            pred = depth_sess.run(None, {depth_in_name: xd})[0]\n",
    "            d = np.asarray(pred)\n",
    "            if d.ndim == 4: d = d[0, 0 if d.shape[1] >= 1 else 0, :, :]\n",
    "            elif d.ndim == 3: d = d[0, :, :]\n",
    "            d = np.maximum(d.astype(np.float32), 1e-6)\n",
    "            inv_full = cv2.resize(d, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "            if inv_depth_full is None:\n",
    "                inv_depth_full = inv_full\n",
    "            else:\n",
    "                a = float(np.clip(args.depth_ema, 0.0, 0.99))\n",
    "                inv_depth_full = a * inv_depth_full + (1.0 - a) * inv_full\n",
    "\n",
    "        # overlay + free space\n",
    "        vis = colorize(mask)\n",
    "        overlay = cv2.addWeighted(frame, 1.0 - args.alpha, vis, args.alpha, 0.0)\n",
    "        if fs_ids:\n",
    "            free = np.isin(mask, np.array(fs_ids, dtype=np.uint8))\n",
    "            free_u8 = (free.astype(np.uint8) * 255)\n",
    "            if kernel is not None:\n",
    "                free_u8 = cv2.morphologyEx(free_u8, cv2.MORPH_OPEN, kernel)\n",
    "                free_u8 = cv2.morphologyEx(free_u8, cv2.MORPH_CLOSE, kernel)\n",
    "            idx = free_u8 == 255\n",
    "            if args.free_alpha > 0:\n",
    "                overlay[idx] = (0.8 * overlay[idx] + np.array([0,255,0]) * args.free_alpha).astype(np.uint8)\n",
    "\n",
    "        # detections\n",
    "        dets_to_draw = last_dets\n",
    "        if det_sess is not None and (frame_idx % max(1, args.det_interval) == 0):\n",
    "            yimg, ratio, pad, orig_hw = yolo_preprocess_bgr(frame, size=args.det_size)\n",
    "            pred = det_sess.run(None, {det_in_name: yimg})[0]\n",
    "            raw = yolo_decode(pred, ratio, pad, orig_hw, conf_thres=args.det_thresh,\n",
    "                              iou_thres=args.det_iou, classes=det_classes_idx, max_det=100)\n",
    "            dets = []\n",
    "            for (x1, y1, x2, y2), score, cid in raw:\n",
    "                dist_m = None\n",
    "                if has_depth and inv_depth_full is not None and k_scale is not None:\n",
    "                    cx = int((x1 + x2) / 2.0)\n",
    "                    cy = int(min(inv_depth_full.shape[0]-1, y2))\n",
    "                    xlo = max(0, cx - 2); xhi = min(inv_depth_full.shape[1], cx + 3)\n",
    "                    ylo = max(0, cy - 4); yhi = min(inv_depth_full.shape[0], cy + 1)\n",
    "                    patch = inv_depth_full[ylo:yhi, xlo:xhi]\n",
    "                    if patch.size > 0:\n",
    "                        inv_med = float(np.median(patch))\n",
    "                        if inv_med > 1e-6:\n",
    "                            dist_m = float(np.clip(k_scale / inv_med, 0.3, 20.0))\n",
    "                if dist_m is None or dist_m <= args.max_distance:\n",
    "                    dets.append(((int(x1), int(y1), int(x2), int(y2)), score, cid, dist_m))\n",
    "            dets_to_draw = dets\n",
    "            last_dets = dets\n",
    "\n",
    "        # draw + speech candidates\n",
    "        speak_set = set(n.lower() for n in args.speak_classes) if args.speak_classes else set()\n",
    "        detections = []  # (name, box, dist)\n",
    "        for (x1, y1, x2, y2), score, cid, dist_m in dets_to_draw:\n",
    "            name = COCO80[cid] if 0 <= cid < len(COCO80) else f\"id{cid}\"\n",
    "            cv2.rectangle(overlay, (x1, y1), (x2, y2), (255,255,255), 2, cv2.LINE_AA)\n",
    "            label = f\"{name} {score:.2f}\"\n",
    "            if dist_m is not None: label += f\" — {dist_m:.1f} m\"\n",
    "            ty = max(14, y1 - 6)\n",
    "            cv2.putText(overlay, label, (x1, ty), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA)\n",
    "            if (not speak_set) or (name.lower() in speak_set):\n",
    "                detections.append((name, (x1,y1,x2,y2), dist_m))\n",
    "\n",
    "        # speech logic (unchanged from your version)\n",
    "        phrases = []\n",
    "        tnow = time.time()\n",
    "\n",
    "        # simple track store\n",
    "        def iou_xyxy(a, b):\n",
    "            ax1, ay1, ax2, ay2 = a; bx1, by1, bx2, by2 = b\n",
    "            ix1, iy1 = max(ax1, bx1), max(ay1, by1)\n",
    "            ix2, iy2 = min(ax2, bx2), min(ay2, by2)\n",
    "            iw, ih = max(0, ix2 - ix1), max(0, iy2 - iy1)\n",
    "            inter = iw * ih\n",
    "            if inter == 0: return 0.0\n",
    "            aarea = (ax2 - ax1) * (ay2 - ay1)\n",
    "            barea = (bx2 - bx1) * (by2 - by1)\n",
    "            return inter / (aarea + barea - inter + 1e-6)\n",
    "\n",
    "        def match_track_iou(tracks, name, box, iou_thresh=0.3):\n",
    "            best_k, best_iou = None, iou_thresh\n",
    "            for k, t in tracks.items():\n",
    "                if t[\"name\"] != name: continue\n",
    "                iou = iou_xyxy(t[\"box\"], box)\n",
    "                if iou > best_iou:\n",
    "                    best_iou, best_k = iou, k\n",
    "            return best_k\n",
    "\n",
    "        if args.speak and args.speak_once:\n",
    "            stale = [k for k,t in tracks.items() if (frame_idx - t[\"last_seen\"]) > int(args.speak_miss_frames)]\n",
    "            for k in stale: del tracks[k]\n",
    "\n",
    "            for name, box, dist_m in detections:\n",
    "                if (tnow - last_spoken_per_class.get(name, 0.0)) < float(args.speak_per_class_gap):\n",
    "                    k = match_track_iou(tracks, name, box, iou_thresh=0.35)\n",
    "                    if k is None:\n",
    "                        tid = max(tracks.keys(), default=0) + 1\n",
    "                        tracks[tid] = {\"name\": name, \"box\": box, \"last_seen\": frame_idx,\n",
    "                                       \"last_dist\": dist_m if dist_m is not None else float('inf'),\n",
    "                                       \"announced_at\": 0.0}\n",
    "                    else:\n",
    "                        t = tracks[k]; t[\"box\"] = box; t[\"last_seen\"] = frame_idx\n",
    "                        if dist_m is not None: t[\"last_dist\"] = dist_m\n",
    "                    continue\n",
    "\n",
    "                k = match_track_iou(tracks, name, box, iou_thresh=0.35)\n",
    "                if k is None:\n",
    "                    if (tnow - last_any_spoken) >= float(args.speak_min_gap):\n",
    "                        phrases.append(f\"{name} {dist_m:.1f} meters\" if (dist_m is not None and np.isfinite(dist_m)) else f\"{name}\")\n",
    "                        last_spoken_per_class[name] = tnow\n",
    "                        last_any_spoken = tnow\n",
    "                    tid = max(tracks.keys(), default=0) + 1\n",
    "                    tracks[tid] = {\"name\": name, \"box\": box, \"last_seen\": frame_idx,\n",
    "                                   \"last_dist\": dist_m if dist_m is not None else float('inf'),\n",
    "                                   \"announced_at\": tnow}\n",
    "                else:\n",
    "                    t = tracks[k]\n",
    "                    t[\"box\"] = box; t[\"last_seen\"] = frame_idx\n",
    "                    if dist_m is not None and np.isfinite(dist_m):\n",
    "                        prev = t.get(\"last_dist\", float('inf'))\n",
    "                        rel_drop = (prev - dist_m) / max(prev, 1e-6)\n",
    "                        abs_drop = prev - dist_m\n",
    "                        if (tnow - t.get(\"announced_at\", 0.0)) >= float(args.reannounce_time) and \\\n",
    "                           (rel_drop >= float(args.reannounce_drop) or abs_drop >= float(args.reannounce_abs)) and \\\n",
    "                           (tnow - last_any_spoken) >= float(args.speak_min_gap):\n",
    "                            phrases.append(f\"{name} {dist_m:.1f} meters\")\n",
    "                            last_spoken_per_class[name] = tnow\n",
    "                            last_any_spoken = tnow\n",
    "                            t[\"announced_at\"] = tnow\n",
    "                        t[\"last_dist\"] = dist_m\n",
    "        elif args.speak:\n",
    "            detections.sort(key=lambda d: (0 if (d[2] is not None and np.isfinite(d[2])) else 1,\n",
    "                                           d[2] if d[2] is not None else 1e9))\n",
    "            for name, _, dist_m in detections[:max(1, int(args.max_announce))]:\n",
    "                if (tnow - last_spoken_per_class.get(name, 0.0)) >= float(args.cooldown) and \\\n",
    "                   (tnow - last_any_spoken) >= float(args.speak_min_gap):\n",
    "                    phrases.append(f\"{name} {dist_m:.1f} meters\" if (dist_m is not None and np.isfinite(dist_m)) else f\"{name}\")\n",
    "                    last_spoken_per_class[name] = tnow\n",
    "                    last_any_spoken = tnow\n",
    "\n",
    "        if args.speak and phrases:\n",
    "            sentence = \", \".join([p.capitalize() for p in phrases]) + \" ahead\"\n",
    "            print(\"[announce]\", sentence, flush=True)\n",
    "            speak(sentence, enable=True)\n",
    "\n",
    "        # HUD\n",
    "        if has_depth:\n",
    "            calib_txt = f\"Calib: {'OK' if k_scale is not None else 'Press c'}\"\n",
    "            cv2.putText(overlay, calib_txt, (12, 52),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # FPS\n",
    "        frames += 1\n",
    "        now = time.time()\n",
    "        if args.show_fps and (now - last_fps_t) >= 1.0:\n",
    "            fps = frames / (now - last_fps_t)\n",
    "            last_fps_t = now; frames = 0\n",
    "            cv2.putText(overlay, f\"FPS: {fps:.1f}\", (12, 28),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # show\n",
    "        cv2.imshow('SemNav — Webcam', overlay)\n",
    "        if writer: writer.write(overlay)\n",
    "\n",
    "        # keys\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key in (27, ord('q')):\n",
    "            break\n",
    "        if key == ord('c') and has_depth and (inv_depth_full is not None):\n",
    "            Hc, Wc = inv_depth_full.shape\n",
    "            cx1, cy1 = int(Wc*0.45), int(Hc*0.45)\n",
    "            cx2, cy2 = int(Wc*0.55), int(Hc*0.55)\n",
    "            roi = inv_depth_full[cy1:cy2, cx1:cx2]\n",
    "            if roi.size > 0:\n",
    "                inv_med = float(np.median(roi))\n",
    "                if inv_med > 1e-6:\n",
    "                    k_scale = float(args.calib_distance) * inv_med\n",
    "                    print(f\"[calib] k_scale set to {k_scale:.4f} (maps center ROI to {args.calib_distance} m)\", flush=True)\n",
    "                    speak(\"Calibration set\", enable=args.speak)\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    if writer: writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Webcam demo finished.\", flush=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad001871-f1eb-4df4-bcf9-99c04d39b57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jayamdaxeshkumarshah/semnav_terrain\n",
      "[announce] Person ahead\n",
      "[calib] k_scale set to 1712.0177 (maps center ROI to 2.0 m)\n",
      "[announce] Person 2.1 meters ahead\n",
      "[announce] Chair 2.0 meters ahead\n",
      "[announce] Cup 2.6 meters ahead\n",
      "Webcam demo finished.\n"
     ]
    }
   ],
   "source": [
    "%cd ~/semnav_terrain\n",
    "\n",
    "!python -u webcam_demo.py \\\n",
    "  --weights \"$PWD/export/model.onnx\" \\\n",
    "  --backend onnx \\\n",
    "  --input-size 512 384 \\\n",
    "  --classes ground sidewalk stairs water person car sky \\\n",
    "  --free-space ground sidewalk \\\n",
    "  --pseudo-depth \"$PWD/models/midas_small.onnx\" \\\n",
    "  --det-onnx    \"$PWD/models/yolov5n.onnx\" \\\n",
    "  --det-size 640 --det-interval 2 --det-thresh 0.35 \\\n",
    "  --max-distance 5.0 \\\n",
    "  --speak \\\n",
    "  --speak-classes \\\n",
    "  person bicycle car motorcycle airplane bus train truck boat \"notebook\" \"traffic light\" \"fire hydrant\" \"stop sign\" \"parking meter\" bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack umbrella handbag tie suitcase frisbee skis snowboard \"sports ball\" kite \"baseball bat\" \"baseball glove\" skateboard surfboard \"tennis racket\" bottle \"wine glass\" cup fork knife spoon bowl banana apple sandwich orange broccoli carrot \"hot dog\" pizza donut cake chair couch \"potted plant\" bed \"dining table\" toilet tv laptop mouse remote keyboard \"cell phone\" microwave oven toaster sink refrigerator book clock vase scissors \"teddy bear\" \"hair drier\" toothbrush \\\n",
    "  --cooldown 8 --speak-min-gap 1.5 --speak-per-class-gap 7 \\\n",
    "  --speak-miss-frames 90 --reannounce-drop 0.6 --reannounce-abs 1.5 --reannounce-time 12 \\\n",
    "  --alpha 0.35 --free-alpha 0.10 --show-fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab7f257a-9c7e-4763-bee5-4f026fe23c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1337  100  1337    0     0   6037      0 --:--:-- --:--:-- --:--:--  6049\n",
      "100 63.3M  100 63.3M    0     0  46.8M      0  0:00:01  0:00:01 --:--:-- 73.6M\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p semnav_terrain/models\n",
    "!curl -L \"https://huggingface.co/julienkay/sentis-MiDaS/resolve/main/onnx/midas_v21_small_256.onnx\" -o semnav_terrain/models/midas_small.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72a2bd0-e9f7-45ed-8a89-6533dac00d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/jayamdaxeshkumarshah/semnav_terrain/Makefile\n"
     ]
    }
   ],
   "source": [
    "# Writes a Makefile with simple CPU targets.\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mk = textwrap.dedent(\"\"\"\\\n",
    ".PHONY: env train export demo\n",
    "\n",
    "env:\n",
    "\\tpip install -r requirements.txt\n",
    "\n",
    "train:\n",
    "\\tpython train.py --cfg configs/deeplabv3_mbv3.yaml --fold 0\n",
    "\n",
    "export:\n",
    "\\tpython infer.py --weights runs/fold0/best.pth --export onnx --output export/model.onnx\n",
    "\n",
    "demo:\n",
    "\\tpython webcam_demo.py --weights export/model.onnx --backend onnx --input-size 512 384 --classes ground sidewalk stairs water person car sky --free-space ground sidewalk --show-fps --save-video demo_out.mp4\n",
    "\"\"\")\n",
    "(root / \"Makefile\").write_text(mk)\n",
    "print(\"Wrote:\", (root / \"Makefile\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0affa582-2677-47c3-949a-348e0d7bbe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: /Users/jayamdaxeshkumarshah/semnav_terrain/README.md\n"
     ]
    }
   ],
   "source": [
    "# Appends a CPU quickstart to README.md (creates file if missing).\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "readme_path = root / \"README.md\"\n",
    "\n",
    "quickstart = textwrap.dedent(\"\"\"\\n\n",
    "## Quickstart (CPU)\n",
    "```bash\n",
    "make env\n",
    "make train\n",
    "make export\n",
    "make demo\n",
    "\"\"\")\n",
    "\n",
    "if readme_path.exists():\n",
    "    readme_path.write_text(readme_path.read_text() + quickstart)\n",
    "else:\n",
    "    readme_path.write_text(\"# SemNav Terrain\\n\" + quickstart)\n",
    "\n",
    "print(\"Updated:\", readme_path.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6ba7158-39eb-4b98-acd1-a7214b2b2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceptance Checklist (Webcam Demo)\n",
      "[ ] Mean mIoU ≥ 55% (5-fold) for RGB baseline  --> train longer/more folds\n",
      "[x] Webcam overlay available (ONNX exported)\n",
      "[x] Free-space mask rendered and reasonably stable\n",
      "[ ] (Optional) Pseudo-depth Recall@2m displayed (skipped)\n",
      "[x] Exported ONNX & TorchScript exist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prints a minimal status against the webcam demo checklist.\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "ok_onnx   = (root / \"export\" / \"model.onnx\").exists()\n",
    "ok_ts     = (root / \"export\" / \"model.ts\").exists()\n",
    "ok_export = ok_onnx and ok_ts\n",
    "ok_metrics= (root / \"runs\" / \"fold0\" / \"val_metrics.csv\").exists()\n",
    "ok_free   = True  # free-space overlay renders if you ran the webcam demo\n",
    "\n",
    "print(\"Acceptance Checklist (Webcam Demo)\")\n",
    "print(\"[ ] Mean mIoU ≥ 55% (5-fold) for RGB baseline  --> train longer/more folds\")\n",
    "print(f\"[{'x' if ok_onnx else ' '}] Webcam overlay available (ONNX exported)\")\n",
    "print(f\"[{'x' if ok_free else ' '}] Free-space mask rendered and reasonably stable\")\n",
    "print(\"[ ] (Optional) Pseudo-depth Recall@2m displayed (skipped)\")\n",
    "print(f\"[{'x' if ok_export else ' '}] Exported ONNX & TorchScript exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61ea0635-c27c-4d19-937c-7f7ac60d5db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: semnav_terrain/handover_bundle.zip\n"
     ]
    }
   ],
   "source": [
    "# What this cell does:\n",
    "# - Zips key artifacts: checkpoints, export models, reports, demo video (if exists).\n",
    "\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"semnav_terrain\")\n",
    "bundle = root / \"handover_bundle.zip\"\n",
    "with zipfile.ZipFile(bundle, 'w', zipfile.ZIP_DEFLATED) as z:\n",
    "    for p in [\n",
    "        root / \"runs\" / \"fold0\" / \"best.pth\",\n",
    "        root / \"export\" / \"model.onnx\",\n",
    "        root / \"export\" / \"model.ts\",\n",
    "        root / \"reports\" / \"summary.csv\",\n",
    "    ]:\n",
    "        if p.exists():\n",
    "            z.write(p, p.relative_to(root).as_posix())\n",
    "    # include figures and optional demo video\n",
    "    for fig in (root / \"reports\" / \"figs\").glob(\"*.png\"):\n",
    "        z.write(fig, fig.relative_to(root).as_posix())\n",
    "    demo = root / \"demo_out.mp4\"\n",
    "    if demo.exists():\n",
    "        z.write(demo, demo.relative_to(root).as_posix())\n",
    "\n",
    "print(\"Created:\", bundle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
